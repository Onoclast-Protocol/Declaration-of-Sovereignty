# Copyright (c) 2024 James Chapman
#
# This software is dual-licensed:
#
# - For individuals and non-commercial use: Licensed under the MIT License.
# - For commercial or corporate use: A separate commercial license is required.
#
# To obtain a commercial license, please contact: iconoclastdao@gmail.com
#
# By using this software, you agree to these terms.
#
# MIT License (for individuals and non-commercial use):
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import json
import time
import hashlib
import base64
import argparse
import struct
from typing import List, Dict, Any, Tuple, Optional
from abc import ABC, abstractmethod
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
import random

class TCCLogger:
    def __init__(self):
        self.tcc_log: List[TCCLogEntry] = []
        self.step_counter: int = 0

    def log(self, operation: str, input_data: bytes, output_data: bytes, metadata: Dict[str, Any] = None, log_level: str = "INFO", error_code: str = "NONE") -> None:
        entry = TCCLogEntry(self.step_counter, operation, input_data, output_data, metadata or {}, log_level, error_code, prev_hash=self._compute_prev_hash())
        self.tcc_log.append(entry)
        self.step_counter += 1

    def _compute_prev_hash(self) -> bytes:
        if not self.tcc_log:
            return b'\x00' * 32
        last_entry = self.tcc_log[-1]
        return hashlib.sha256(last_entry.to_bytes()).digest()

    def save_log(self, filename: str) -> None:
        with open(filename, 'w') as f:
            for entry in self.tcc_log:
                f.write(json.dumps(entry.to_json()) + '\n')

class TCCLogEntry:
    def __init__(self, step: int, operation: str, input_data: bytes, output_data: bytes, metadata: Dict[str, Any], log_level: str, error_code: str, prev_hash: bytes):
        self.step = step
        self.operation = operation
        self.input_data = input_data
        self.output_data = output_data
        self.metadata = metadata
        self.log_level = log_level
        self.error_code = error_code
        self.prev_hash = prev_hash
        self.operation_id = hashlib.sha256(f"{step}:{operation}:{time.time_ns()}".encode()).hexdigest()[:32]
        self.timestamp = time.time_ns()
        self.execution_time_ns = 0

    def to_bytes(self) -> bytes:
        start_time = time.time_ns()
        step_bytes = struct.pack('>I', self.step)
        op_bytes = self.operation.encode('utf-8').ljust(32, b'\x00')[:32]
        input_len = len(self.input_data)
        output_len = len(self.output_data)
        input_len_bytes = struct.pack('>I', input_len)
        output_len_bytes = struct.pack('>I', output_len)
        meta_bytes = json.dumps(self.metadata).encode('utf-8').ljust(128, b'\x00')[:128]
        level_bytes = self.log_level.encode('utf-8').ljust(16, b'\x00')[:16]
        error_bytes = self.error_code.encode('utf-8').ljust(16, b'\x00')[:16]
        op_id_bytes = self.operation_id.encode('utf-8').ljust(32, b'\x00')[:32]
        ts_bytes = struct.pack('>Q', self.timestamp)
        exec_time_bytes = struct.pack('>Q', self.execution_time_ns)
        result = (step_bytes + op_bytes + input_len_bytes + self.input_data +
                  output_len_bytes + self.output_data + meta_bytes + level_bytes +
                  error_bytes + self.prev_hash + op_id_bytes + ts_bytes + exec_time_bytes)
        self.execution_time_ns = time.time_ns() - start_time
        return result

    def to_json(self) -> Dict[str, Any]:
        return {
            "step": self.step,
            "operation": self.operation,
            "input_data": base64.b64encode(self.input_data).decode('utf-8'),
            "output_data": base64.b64encode(self.output_data).decode('utf-8'),
            "metadata": self.metadata,
            "log_level": self.log_level,
            "error_code": self.error_code,
            "prev_hash": base64.b64encode(self.prev_hash).decode('utf-8'),
            "operation_id": self.operation_id,
            "timestamp": self.timestamp,
            "execution_time_ns": self.execution_time_ns
        }

class LLMModule(ABC):
    def __init__(self):
        self.logger = TCCLogger()

    @abstractmethod
    def compute(self, input_data: Any) -> Any:
        pass

    @abstractmethod
    def reverse(self, output_data: Any) -> Any:
        pass

    @abstractmethod
    def mimic_transformation(self, input_data: Any, ref_input: Any, ref_output: Any) -> Any:
        pass

class ModelManager:
    def __init__(self, model_name: str = "distilgpt2"):
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)
        self.model.eval()

class TokenizerModule(LLMModule):
    def __init__(self, model_manager: ModelManager):
        super().__init__()
        self.tokenizer = model_manager.tokenizer

    def compute(self, input_text: str) -> Dict[str, Any]:
        start_time = time.time_ns()
        tokens = self.tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)
        token_ids = tokens["input_ids"].numpy().tobytes()
        attention_mask = tokens["attention_mask"].numpy().tobytes()
        output = {"input_ids": token_ids, "attention_mask": attention_mask}
        self.logger.log(
            "tokenize",
            input_text.encode('utf-8'),
            json.dumps(output).encode('utf-8'),
            {"token_count": len(tokens["input_ids"][0]), "execution_time_ns": time.time_ns() - start_time}
        )
        return output

    def reverse(self, output_data: Dict[str, Any]) -> str:
        start_time = time.time_ns()
        input_ids = np.frombuffer(output_data["input_ids"], dtype=np.int64).reshape(1, -1)
        text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
        self.logger.log(
            "reverse_tokenize",
            json.dumps(output_data).encode('utf-8'),
            text.encode('utf-8'),
            {"execution_time_ns": time.time_ns() - start_time}
        )
        return text

    def mimic_transformation(self, input_text: str, ref_input: str, ref_output: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time_ns()
        adjusted_text = input_text[:len(ref_input)] if len(input_text) > len(ref_input) else input_text + " " * (len(ref_input) - len(input_text))
        output = self.compute(adjusted_text)
        self.logger.log(
            "mimic_tokenize",
            input_text.encode('utf-8'),
            json.dumps(output).encode('utf-8'),
            {"ref_input": ref_input, "adjusted_text": adjusted_text}
        )
        return output

class EmbedderModule(LLMModule):
    def __init__(self, model_manager: ModelManager):
        super().__init__()
        self.model = model_manager.model
        self.embedding_layer = self.model.get_input_embeddings()

    def compute(self, tokens: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time_ns()
        input_ids = torch.from_numpy(np.frombuffer(tokens["input_ids"], dtype=np.int64).reshape(1, -1))
        embeddings = self.embedding_layer(input_ids).detach().numpy().tobytes()
        output = {"embeddings": embeddings, "input_ids": tokens["input_ids"]}
        self.logger.log(
            "embed",
            tokens["input_ids"],
            embeddings,
            {"shape": str(np.frombuffer(embeddings).shape), "execution_time_ns": time.time_ns() - start_time}
        )
        return output

    def reverse(self, output_data: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time_ns()
        input_ids = output_data["input_ids"]
        self.logger.log(
            "reverse_embed",
            output_data["embeddings"],
            input_ids,
            {"execution_time_ns": time.time_ns() - start_time}
        )
        return {"input_ids": input_ids}

    def mimic_transformation(self, tokens: Dict[str, Any], ref_input: Dict[str, Any], ref_output: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time_ns()
        output = self.compute(tokens)
        self.logger.log(
            "mimic_embed",
            tokens["input_ids"],
            output["embeddings"],
            {"ref_input_ids": ref_input["input_ids"].hex()}
        )
        return output

class TransformerLayerModule(LLMModule):
    def __init__(self, model_manager: ModelManager, layer_idx: int = 0):
        super().__init__()
        self.model = model_manager.model
        self.layer = self.model.transformer.h[layer_idx]
        self.layer_idx = layer_idx

    def compute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time_ns()
        embeddings = torch.from_numpy(np.frombuffer(input_data["embeddings"]).reshape(1, -1, self.model.config.hidden_size))
        input_ids = input_data["input_ids"]
        attention_mask = torch.from_numpy(np.frombuffer(input_data.get("attention_mask", b""), dtype=np.int64).reshape(1, -1))
        outputs = self.layer(embeddings, attention_mask=attention_mask)[0].detach().numpy().tobytes()
        output = {"hidden_states": outputs, "input_ids": input_ids}
        self.logger.log(
            "transformer_layer",
            input_data["embeddings"],
            outputs,
            {"layer_idx": self.layer_idx, "execution_time_ns": time.time_ns() - start_time}
        )
        return output

    def reverse(self, output_data: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time_ns()
        input_ids = output_data["input_ids"]
        self.logger.log(
            "reverse_transformer_layer",
            output_data["hidden_states"],
            input_ids,
            {"layer_idx": self.layer_idx, "execution_time_ns": time.time_ns() - start_time}
        )
        return {"embeddings": output_data["hidden_states"], "input_ids": input_ids}

    def mimic_transformation(self, input_data: Dict[str, Any], ref_input: Dict[str, Any], ref_output: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time_ns()
        output = self.compute(input_data)
        self.logger.log(
            "mimic_transformer_layer",
            input_data["embeddings"],
            output["hidden_states"],
            {"layer_idx": self.layer_idx, "ref_input_ids": ref_input["input_ids"].hex()}
        )
        return output

class DecoderModule(LLMModule):
    def __init__(self, model_manager: ModelManager, temperature: float = 1.0, top_k: int = 50):
        super().__init__()
        self.model = model_manager.model
        self.tokenizer = model_manager.tokenizer
        self.temperature = temperature
        self.top_k = top_k

    def compute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time_ns()
        hidden_states = torch.from_numpy(np.frombuffer(input_data["hidden_states"]).reshape(1, -1, self.model.config.hidden_size))
        input_ids = torch.from_numpy(np.frombuffer(input_data["input_ids"], dtype=np.int64).reshape(1, -1))
        logits = self.model.lm_head(hidden_states).detach()
        probs = torch.softmax(logits / self.temperature, dim=-1)
        top_k_probs, top_k_indices = torch.topk(probs, self.top_k, dim=-1)
        token_idx = torch.multinomial(top_k_probs[0, -1], 1).item()
        next_token = top_k_indices[0, -1, token_idx].item()
        output_text = self.tokenizer.decode([next_token], skip_special_tokens=True)
        output = {"output_text": output_text, "logits": logits.numpy().tobytes(), "next_token": next_token}
        self.logger.log(
            "decode",
            input_data["hidden_states"],
            output_text.encode('utf-8'),
            {"next_token": next_token, "execution_time_ns": time.time_ns() - start_time}
        )
        return output

    def reverse(self, output_data: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time_ns()
        logits = np.frombuffer(output_data["logits"]).reshape(1, -1, self.model.config.vocab_size)
        hidden_states = self.model.lm_head.weight.data.T @ torch.from_numpy(logits).squeeze(0).T
        self.logger.log(
            "reverse_decode",
            output_data["output_text"].encode('utf-8'),
            hidden_states.numpy().tobytes(),
            {"execution_time_ns": time.time_ns() - start_time}
        )
        return {"hidden_states": hidden_states.numpy().tobytes(), "input_ids": output_data.get("input_ids", b"")}

    def mimic_transformation(self, input_data: Dict[str, Any], ref_input: Dict[str, Any], ref_output: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time_ns()
        output = self.compute(input_data)
        self.logger.log(
            "mimic_decode",
            input_data["hidden_states"],
            output["output_text"].encode('utf-8'),
            {"ref_output_text": ref_output["output_text"]}
        )
        return output

class LLMEntropyEngine:
    FEE_PER_SAMPLING = 1000

    def __init__(self, initial_fee: int = 1000):
        self.fee_per_sampling = initial_fee
        self.commitments = {}
        self.commitment_timestamps = {}
        self.logger = TCCLogger()
        self.random_state = random.Random()

    def commit_sampling(self, user_id: str, seed: int, temperature: float) -> None:
        commitment = hashlib.sha256(f"{seed}:{temperature}".encode()).digest()
        if user_id in self.commitments:
            raise ValueError("Commitment already exists for user")
        self.commitments[user_id] = (seed, temperature)
        self.commitment_timestamps[user_id] = time.time_ns()
        self.logger.log(
            "sampling_committed",
            commitment,
            b"",
            {"user_id": user_id, "seed": seed, "temperature": temperature}
        )

    def reveal_sampling(self, user_id: str, seed: int, temperature: float, fee: int) -> None:
        if fee < self.fee_per_sampling:
            raise ValueError("Insufficient fee")
        if user_id not in self.commitments:
            raise ValueError("No commitment found")
        if self.commitments[user_id] != (seed, temperature):
            raise ValueError("Invalid commitment")
        if (time.time_ns() - self.commitment_timestamps[user_id]) > 86400 * 1_000_000_000:
            raise ValueError("Commitment expired")
        del self.commitments[user_id]
        del self.commitment_timestamps[user_id]
        self.random_state.seed(seed)
        self.logger.log(
            "sampling_revealed",
            f"{seed}:{temperature}".encode(),
            b"",
            {"user_id": user_id}
        )

    def save_log(self, filename: str) -> None:
        self.logger.save_log(filename)

class LLMCoordinator:
    def __init__(self, engine_a: LLMEntropyEngine, engine_b: LLMEntropyEngine, engine_c: LLMEntropyEngine):
        if not all([engine_a, engine_b, engine_c]) or len(set([id(engine_a), id(engine_b), id(engine_c)])) != 3:
            raise ValueError("Invalid or duplicate engines")
        self.engine_a = engine_a
        self.engine_b = engine_b
        self.engine_c = engine_c
        self.logger = TCCLogger()

    def commit_sampling_all(self, user_id: str, seed_a: int, temp_a: float, seed_b: int, temp_b: float, seed_c: int, temp_c: float) -> None:
        self.engine_a.commit_sampling(user_id, seed_a, temp_a)
        self.engine_b.commit_sampling(user_id, seed_b, temp_b)
        self.engine_c.commit_sampling(user_id, seed_c, temp_c)
        self.logger.log(
            "commit_sampling_all",
            f"{seed_a}:{temp_a}:{seed_b}:{temp_b}:{seed_c}:{temp_c}".encode(),
            b"",
            {"user_id": user_id}
        )

    def reveal_sampling_all(self, user_id: str, seed_a: int, temp_a: float, seed_b: int, temp_b: float, seed_c: int, temp_c: float, fee: int) -> None:
        total_fee = 0
        has_a = user_id in self.engine_a.commitments
        has_b = user_id in self.engine_b.commitments
        has_c = user_id in self.engine_c.commitments
        if has_a:
            total_fee += self.engine_a.fee_per_sampling
        if has_b:
            total_fee += self.engine_b.fee_per_sampling
        if has_c:
            total_fee += self.engine_c.fee_per_sampling
        if fee < total_fee:
            raise ValueError("Insufficient fee")
        if has_a:
            self.engine_a.reveal_sampling(user_id, seed_a, temp_a, self.engine_a.fee_per_sampling)
        if has_b:
            self.engine_b.reveal_sampling(user_id, seed_b, temp_b, self.engine_b.fee_per_sampling)
        if has_c:
            self.engine_c.reveal_sampling(user_id, seed_c, temp_c, self.engine_c.fee_per_sampling)
        self.logger.log(
            "reveal_sampling_all",
            f"{seed_a}:{temp_a}:{seed_b}:{temp_b}:{seed_c}:{temp_c}".encode(),
            b"",
            {"user_id": user_id, "total_fee": total_fee}
        )

    def save_log(self, filename: str) -> None:
        self.logger.save_log(filename)

class LLMFlow:
    def __init__(self, steps: List[Tuple[str, LLMModule, Dict[str, Any]]], reference_input: str):
        self.steps = steps
        self.logger = TCCLogger()
        self.flow_log: List[Dict[str, Any]] = []
        self.reference_input = reference_input
        self.reference_outputs = self._compute_reference_outputs()

    def _compute_reference_outputs(self) -> List[Any]:
        outputs = [self.reference_input]
        current_data = self.reference_input
        for step_name, module, _ in self.steps:
            output_data = module.compute(current_data)
            outputs.append(output_data)
            current_data = output_data
        return outputs

    def execute(self, input_text: str) -> str:
        current_data = input_text
        start_time = time.time_ns()
        for step_idx, (step_name, module, params) in enumerate(self.steps):
            output_data = module.compute(current_data)
            output_bytes = json.dumps(output_data).encode('utf-8') if isinstance(output_data, dict) else output_data.encode('utf-8')
            input_bytes = current_data.encode('utf-8') if isinstance(current_data, str) else json.dumps(current_data).encode('utf-8')
            metadata = {
                "step_index": step_idx,
                "step_name": step_name,
                "params": params,
                "output_type": str(type(output_data))
            }
            self.logger.log(
                f"flow_{step_name}",
                input_bytes,
                output_bytes,
                metadata
            )
            self.flow_log.append({
                "step_index": step_idx,
                "step_name": step_name,
                "operation_id": self.logger.tcc_log[-1].operation_id,
                "input_data": base64.b64encode(input_bytes).decode('utf-8'),
                "output_data": base64.b64encode(output_bytes).decode('utf-8'),
                "timestamp": time.time_ns()
            })
            current_data = output_data
        final_output = current_data["output_text"] if isinstance(current_data, dict) else current_data
        self.logger.log(
            "flow_complete",
            input_text.encode('utf-8'),
            final_output.encode('utf-8'),
            {"total_steps": len(self.steps), "execution_time_ns": time.time_ns() - start_time}
        )
        return final_output

    def reverse(self, target_output: str) -> str:
        start_time = time.time_ns()
        target_bytes = target_output.encode('utf-8')
        for entry in reversed(self.logger.tcc_log):
            if entry.operation == "flow_complete" and entry.output_data == target_bytes:
                current_output = {"output_text": target_output}
                for step_idx in range(len(self.steps) - 1, -1, -1):
                    step_name, module, _ = self.steps[step_idx]
                    current_input = module.reverse(current_output)
                    input_bytes = current_input.encode('utf-8') if isinstance(current_input, str) else json.dumps(current_input).encode('utf-8')
                    output_bytes = json.dumps(current_output).encode('utf-8') if isinstance(current_output, dict) else current_output.encode('utf-8')
                    self.logger.log(
                        f"reverse_{step_name}",
                        output_bytes,
                        input_bytes,
                        {"step_index": step_idx}
                    )
                    current_output = current_input
                final_input = current_output if isinstance(current_output, str) else self.steps[0][1].reverse(current_output)
                self.logger.log(
                    "reverse_complete",
                    target_bytes,
                    final_input.encode('utf-8'),
                    {"reconstructed_input": final_input}
                )
                return final_input
        self.logger.log(
            "reverse",
            target_bytes,
            b"",
            {"error": "Target output not found"},
            "ERROR",
            "NOT_FOUND"
        )
        raise ValueError("Target output not found in log")

    def reverse_arbitrary(self, target_output: str, arbitrary_input: str) -> str:
        start_time = time.time_ns()
        current_data = arbitrary_input
        for step_idx in range(len(self.steps)):
            step_name, module, _ = self.steps[step_idx]
            ref_input = self.reference_outputs[step_idx]
            ref_output = self.reference_outputs[step_idx + 1]
            current_data = module.mimic_transformation(current_data, ref_input, ref_output)
        current_output = {"output_text": target_output}
        for step_idx in range(len(self.steps) - 1, -1, -1):
            step_name, module, _ = self.steps[step_idx]
            current_input = module.reverse(current_output)
            input_bytes = current_input.encode('utf-8') if isinstance(current_input, str) else json.dumps(current_input).encode('utf-8')
            output_bytes = json.dumps(current_output).encode('utf-8') if isinstance(current_output, dict) else current_output.encode('utf-8')
            self.logger.log(
                f"reverse_arbitrary_{step_name}",
                output_bytes,
                input_bytes,
                {"step_index": step_idx, "arbitrary_input": arbitrary_input}
            )
            current_output = current_input
        final_input = current_output if isinstance(current_output, str) else self.steps[0][1].reverse(current_output)
        self.logger.log(
            "reverse_arbitrary_complete",
            target_output.encode('utf-8'),
            final_input.encode('utf-8'),
            {"reconstructed_input": final_input, "arbitrary_input": arbitrary_input}
        )
        return final_input

    def save_flow_log(self, filename: str) -> None:
        with open(filename, 'w') as f:
            for entry in self.flow_log:
                f.write(json.dumps(entry) + '\n')

def define_default_flow(model_name: str = "distilgpt2", num_layers: int = 2) -> LLMFlow:
    reference_input = "Hello, world!"
    model_manager = ModelManager(model_name)
    steps = [
        ("tokenize", TokenizerModule(model_manager), {}),
        ("embed", EmbedderModule(model_manager), {}),
    ]
    for i in range(min(num_layers, 6)):
        steps.append((f"transformer_layer_{i}", TransformerLayerModule(model_manager, i), {}))
    steps.append(("decode", DecoderModule(model_manager), {"temperature": 1.0, "top_k": 50}))
    return LLMFlow(steps, reference_input)

def main():
    parser = argparse.ArgumentParser(description="LLM Flow Demonstrator with Tracing")
    parser.add_argument("--input", type=str, default="Hello, world!", help="Input text")
    parser.add_argument("--arbitrary-input", type=str, help="Arbitrary input for reverse")
    parser.add_argument("--target-output", type=str, help="Target output to reverse")
    parser.add_argument("--model-name", type=str, default="distilgpt2", help="Hugging Face model name")
    parser.add_argument("--log-file", type=str, default="llm_flow_log.jsonl", help="Log file path")
    parser.add_argument("--num-layers", type=int, default=2, help="Number of transformer layers")
    parser.add_argument("--commit-sampling", type=str, help="Commit sampling (seed:temperature)")
    parser.add_argument("--reveal-sampling", type=str, help="Reveal sampling (seed:temperature)")
    parser.add_argument("--user-id", type=str, default="user1", help="User ID for sampling operations")
    parser.add_argument("--fee", type=int, default=1000, help="Fee for sampling reveal")
    args = parser.parse_args()

    input_text = args.input

    engine_a = LLMEntropyEngine()
    engine_b = LLMEntropyEngine()
    engine_c = LLMEntropyEngine()
    coordinator = LLMCoordinator(engine_a, engine_b, engine_c)

    if args.commit_sampling:
        try:
            seed, temp = map(float, args.commit_sampling.split(":"))
            coordinator.commit_sampling_all(args.user_id, int(seed), temp, int(seed), temp, int(seed), temp)
            print(f"Committed sampling for user {args.user_id}: seed={seed}, temp={temp}")
        except ValueError as e:
            print(f"Sampling commit error: {str(e)}")
        return

    if args.reveal_sampling:
        try:
            seed, temp = map(float, args.reveal_sampling.split(":"))
            coordinator.reveal_sampling_all(args.user_id, int(seed), temp, int(seed), temp, int(seed), temp, args.fee)
            print(f"Revealed sampling for user {args.user_id}: seed={seed}, temp={temp}")
        except ValueError as e:
            print(f"Sampling reveal error: {str(e)}")
        return

    flow = define_default_flow(args.model_name, args.num_layers)
    try:
        result = flow.execute(input_text)
        flow.save_flow_log(args.log_file)
        print(f"Input: {input_text}")
        print(f"Output: {result}")

        reconstructed = flow.reverse(result)
        print(f"Reconstructed input: {reconstructed}")
        print(f"Reverse successful: {reconstructed == input_text}")

        if args.arbitrary_input and args.target_output:
            arbitrary_reconstructed = flow.reverse_arbitrary(args.target_output, args.arbitrary_input)
            print(f"Arbitrary input: {args.arbitrary_input}")
            print(f"Target output: {args.target_output}")
            print(f"Arbitrary reconstructed input: {arbitrary_reconstructed}")
    except ValueError as e:
        print(f"Error: {str(e)}")

if __name__ == "__main__":
    main()

# Copyright (c) 2024 Your Name
#
# This software is dual-licensed:
#
# - For individuals and non-commercial use: Licensed under the MIT License.
# - For commercial or corporate use: A separate commercial license is required.
#
# To obtain a commercial license, please contact: iconocastdao@gmail.com
#
# By using this software, you agree to these terms.
#
# MIT License (for individuals and non-commercial use):
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import base64
import json
import time
import struct
import hashlib
import binascii
import argparse
import re
from typing import List, Dict, Any, Tuple
from abc import ABC, abstractmethod
import nacl.signing
import nacl.encoding

class TCCLogger:
    def __init__(self):
        self.tcc_log: List[TCCLogEntry] = []
        self.step_counter: int = 0
        self.signing_key = nacl.signing.SigningKey.generate()
        self.verifying_key = self.signing_key.verify_key

    def log(self, operation: str, input_data: bytes, output_data: bytes, metadata: Dict[str, Any] = None, log_level: str = "INFO", error_code: str = "NONE") -> None:
        entry = TCCLogEntry(
            self.step_counter, operation, input_data, output_data, 
            metadata or {}, log_level, error_code, prev_hash=self._compute_prev_hash(),
            signing_key=self.signing_key
        )
        self.tcc_log.append(entry)
        self.step_counter += 1

    def _compute_prev_hash(self) -> bytes:
        if not self.tcc_log:
            return b'\x00' * 32
        last_entry = self.tcc_log[-1]
        return hashlib.sha256(last_entry.to_bytes()).digest()

    def save_log(self, filename: str) -> None:
        with open(filename, 'w') as f:
            for entry in self.tcc_log:
                f.write(json.dumps(entry.to_json()) + '\n')

class TCCLogEntry:
    def __init__(self, step: int, operation: str, input_data: bytes, output_data: bytes, 
                 metadata: Dict[str, Any], log_level: str, error_code: str, prev_hash: bytes, 
                 signing_key: nacl.signing.SigningKey):
        self.step = step
        self.operation = operation
        self.input_data = input_data
        self.output_data = output_data
        self.metadata = metadata
        self.log_level = log_level
        self.error_code = error_code
        self.prev_hash = prev_hash
        self.operation_id = hashlib.sha256(f"{step}:{operation}:{time.time_ns()}".encode()).hexdigest()[:32]
        self.timestamp = time.time_ns()
        self.execution_time_ns = 0
        self.signature = b''
        entry_bytes = self._to_bytes_without_signature()
        self.signature = signing_key.sign(entry_bytes).signature

    def _to_bytes_without_signature(self) -> bytes:
        step_bytes = struct.pack('>Q', self.step)
        op_bytes = self.operation.encode('utf-8').ljust(32, b'\x00')[:32]
        input_len = len(self.input_data)
        output_len = len(self.output_data)
        input_len_bytes = struct.pack('>I', input_len)
        output_len_bytes = struct.pack('>I', output_len)
        meta_bytes = json.dumps(self.metadata).encode('utf-8').ljust(128, b'\x00')[:128]
        level_bytes = self.log_level.encode('utf-8').ljust(16, b'\x00')[:16]
        error_bytes = self.error_code.encode('utf-8').ljust(16, b'\x00')[:16]
        op_id_bytes = self.operation_id.encode('utf-8').ljust(32, b'\x00')[:32]
        ts_bytes = struct.pack('>Q', self.timestamp)
        exec_time_bytes = struct.pack('>Q', self.execution_time_ns)
        return (step_bytes + op_bytes + input_len_bytes + self.input_data +
                output_len_bytes + self.output_data + meta_bytes + level_bytes +
                error_bytes + self.prev_hash + op_id_bytes + ts_bytes + exec_time_bytes)

    def to_bytes(self) -> bytes:
        start_time = time.time_ns()
        result = self._to_bytes_without_signature() + self.signature
        self.execution_time_ns = time.time_ns() - start_time
        return result

    def to_json(self) -> Dict[str, Any]:
        return {
            "step": self.step,
            "operation": self.operation,
            "input_data": base64.b64encode(self.input_data).decode('utf-8'),
            "output_data": base64.b64encode(self.output_data).decode('utf-8'),
            "metadata": self.metadata,
            "log_level": self.log_level,
            "error_code": self.error_code,
            "prev_hash": binascii.hexlify(self.prev_hash).decode('utf-8'),
            "operation_id": self.operation_id,
            "timestamp": self.timestamp,
            "execution_time_ns": self.execution_time_ns,
            "signature": base64.b64encode(self.signature).decode('utf-8')
        }

    @classmethod
    def from_json(cls, data: Dict[str, Any], verifying_key: nacl.signing.VerifyKey = None) -> 'TCCLogEntry':
        entry = cls(
            step=data["step"],
            operation=data["operation"],
            input_data=base64.b64decode(data["input_data"]),
            output_data=base64.b64decode(data["output_data"]),
            metadata=data["metadata"],
            log_level=data["log_level"],
            error_code=data["error_code"],
            prev_hash=binascii.unhexlify(data["prev_hash"]),
            signing_key=nacl.signing.SigningKey.generate()  # Temporary key
        )
        entry.operation_id = data["operation_id"]
        entry.timestamp = data["timestamp"]
        entry.execution_time_ns = data["execution_time_ns"]
        entry.signature = base64.b64decode(data["signature"])
        if verifying_key:
            try:
                verifying_key.verify(entry._to_bytes_without_signature(), entry.signature)
            except nacl.exceptions.BadSignatureError:
                raise ValueError("Invalid signature in log entry")
        return entry

class TCCAlgorithm(ABC):
    def __init__(self):
        self.logger = TCCLogger()

    @abstractmethod
    def compute(self, input_data: bytes) -> bytes:
        pass

    @abstractmethod
    def reverse(self, target_output: bytes) -> bytes:
        pass

    @abstractmethod
    def mimic_transformation(self, input_data: bytes, ref_input: bytes, ref_output: bytes) -> bytes:
        pass

class TCCSHA256(TCCAlgorithm):
    def __init__(self):
        super().__init__()
        self.reset()

    def reset(self) -> None:
        self.h = [
            0x6a09e667, 0xbb67ae85, 0x3c6ef372, 0xa54ff53a,
            0x510e527f, 0x9b05688c, 0x1f83d9ab, 0x5be0cd19
        ]
        self.k = [
            0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5, 0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,
            0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3, 0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,
            0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc, 0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,
            0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7, 0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,
            0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13, 0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,
            0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3, 0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,
            0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5, 0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,
            0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208, 0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2
      ]

    def _right_rotate(self, x: int, n: int) -> int:
        start_time = time.time_ns()
        result = ((x >> n) | (x << (32 - n))) & 0xFFFFFFFF
        self.logger.log(
            "right_rotate",
            struct.pack('>II', x, n),
            struct.pack('>I', result),
            {"bits": n},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return result

    def _right_shift(self, x: int, n: int) -> int:
        start_time = time.time_ns()
        result = (x >> n) & 0xFFFFFFFF
        self.logger.log(
            "right_shift",
            struct.pack('>II', x, n),
            struct.pack('>I', result),
            {"bits": n},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return result

    def _pad_message(self, message: bytes) -> bytes:
        start_time = time.time_ns()
        if not isinstance(message, bytes):
            self.logger.log("pad_message", b"", b"", {"error": "Message must be bytes"}, "ERROR", "INVALID_INPUT")
            raise ValueError("Message must be bytes")
        msg_len = len(message) * 8
        padded = message + b'\x80'
        while (len(padded) % 64) != 56:
            padded += b'\x00'
        padded += struct.pack('>Q', msg_len)
        self.logger.log(
            "pad_message",
            message,
            padded,
            {"original_length_bits": msg_len},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return padded

    def compute(self, message: bytes) -> bytes:
        start_time = time.time_ns()
        self.reset()
        padded = self._pad_message(message)
        for i in range(0, len(padded), 64):
            chunk = padded[i:i+64]
            w = [0] * 64
            for j in range(16):
                w[j] = struct.unpack('>I', chunk[j*4:j*4+4])[0]
                self.logger.log(
                    "word_init",
                    chunk[j*4:j*4+4],
                    struct.pack('>I', w[j]),
                    {"word_index": j},
                    "INFO",
                    "SUCCESS"
                )
            for j in range(16, 64):
                s0 = self._right_rotate(w[j-15], 7) ^ self._right_rotate(w[j-15], 18) ^ self._right_shift(w[j-15], 3)
                s1 = self._right_rotate(w[j-2], 17) ^ self._right_rotate(w[j-2], 19) ^ self._right_shift(w[j-2], 10)
                w[j] = (w[j-16] + s0 + w[j-7] + s1) & 0xFFFFFFFF
                self.logger.log(
                    "word_extend",
                    struct.pack('>IIII', w[j-16], s0, w[j-7], s1),
                    struct.pack('>I', w[j]),
                    {"word_index": j},
                    "INFO",
                    "SUCCESS"
                )
            a, b, c, d, e, f, g, h = self.h
            for j in range(64):
                S1 = self._right_rotate(e, 6) ^ self._right_rotate(e, 11) ^ self._right_rotate(e, 25)
                ch = (e & f) ^ (~e & g)
                temp1 = (h + S1 + ch + self.k[j] + w[j]) & 0xFFFFFFFF
                S0 = self._right_rotate(a, 2) ^ self._right_rotate(a, 13) ^ self._right_rotate(a, 22)
                maj = (a & b) ^ (a & c) ^ (b & c)
                temp2 = (S0 + maj) & 0xFFFFFFFF
                self.logger.log(
                    "compress",
                    struct.pack('>IIIIIIII', a, b, c, d, e, f, g, h),
                    struct.pack('>IIIIIIII', temp1+temp2, a, b, c, d+temp1, e, f, g),
                    {"round": j, "S0": hex(S0), "S1": hex(S1), "ch": hex(ch), "maj": hex(maj), "temp1": hex(temp1), "temp2": hex(temp2)},
                    "INFO",
                    "SUCCESS"
                )
                h = g
                g = f
                f = e
                e = (d + temp1) & 0xFFFFFFFF
                d = c
                c = b
                b = a
                a = (temp1 + temp2) & 0xFFFFFFFF
            for j, (old, new) in enumerate(zip(self.h, [a, b, c, d, e, f, g, h])):
                self.h[j] = (old + new) & 0xFFFFFFFF
                self.logger.log(
                    "update_hash",
                    struct.pack('>II', old, new),
                    struct.pack('>I', self.h[j]),
                    {"hash_index": j},
                    "INFO",
                    "SUCCESS"
                )
        result = b''.join(struct.pack('>I', h) for h in self.h)
        self.logger.log(
            "finalize",
            b'',
            result,
            {"hash_values": [hex(h) for h in self.h]},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return result

    def reverse(self, target_output: bytes) -> bytes:
        start_time = time.time_ns()
        if not isinstance(target_output, bytes) or len(target_output) != 32:
            self.logger.log("reverse", target_output, b"", {"error": "Invalid target hash"}, "ERROR", "INVALID_INPUT")
            raise ValueError("Target hash must be 32 bytes")
        for entry in reversed(self.logger.tcc_log):
            if entry.operation == "finalize" and entry.output_data == target_output:
                for e in reversed(self.logger.tcc_log):
                    if e.operation == "pad_message":
                        self.logger.log(
                            "reverse_complete",
                            target_output,
                            e.input_data,
                            {"reconstructed_length": len(e.input_data)},
                            "INFO",
                            "SUCCESS"
                        )
                        self.logger.execution_time_ns = time.time_ns() - start_time
                        return e.input_data
        self.logger.log("reverse", target_output, b"", {"error": "Target hash not found"}, "ERROR", "NOT_FOUND")
        self.logger.execution_time_ns = time.time_ns() - start_time
        raise ValueError("Target hash not found in log")

    def mimic_transformation(self, input_data: bytes, ref_input: bytes, ref_output: bytes) -> bytes:
        start_time = time.time_ns()
        adjusted_input = input_data
        if len(input_data) < len(ref_input):
            adjusted_input = input_data + b'\x00' * (len(ref_input) - len(input_data))
        elif len(input_data) > len(ref_input):
            adjusted_input = input_data[:len(ref_input)]
        output = self.compute(adjusted_input)
        self.logger.log(
            "mimic_sha256",
            input_data,
            output,
            {"adjusted_input": adjusted_input.hex(), "ref_input": ref_input.hex(), "ref_output": ref_output.hex()},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return output

class TCCEd25519(TCCAlgorithm):
    def __init__(self, private_key: bytes):
        super().__init__()
        if len(private_key) != 32:
            raise ValueError("Private key must be 32 bytes")
        self.private_key = private_key
        self.signing_key = nacl.signing.SigningKey(private_key)
        self.verifying_key = self.signing_key.verify_key

    def compute(self, message: bytes) -> bytes:
        start_time = time.time_ns()
        if not isinstance(message, bytes):
            self.logger.log("compute", b"", b"", {"error": "Message must be bytes"}, "ERROR", "INVALID_INPUT")
            raise ValueError("Message must be bytes")
        signature = self.signing_key.sign(message).signature
        self.logger.log(
            "sign",
            message,
            signature,
            {"verifying_key": self.verifying_key.encode().hex()},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return signature

    def reverse(self, signature: bytes) -> bytes:
        start_time = time.time_ns()
        if not isinstance(signature, bytes) or len(signature) != 64:
            self.logger.log("reverse", signature, b"", {"error": "Invalid signature"}, "ERROR", "INVALID_INPUT")
            raise ValueError("Signature must be 64 bytes")
        for entry in reversed(self.logger.tcc_log):
            if entry.operation == "sign" and entry.output_data == signature:
                try:
                    self.verifying_key.verify(entry.input_data, signature)
                    self.logger.log(
                        "reverse_complete",
                        signature,
                        entry.input_data,
                        {"reconstructed_length": len(entry.input_data)},
                        "INFO",
                        "SUCCESS"
                    )
                    self.logger.execution_time_ns = time.time_ns() - start_time
                    return entry.input_data
                except nacl.exceptions.BadSignatureError:
                    self.logger.log(
                        "reverse",
                        signature,
                        b"",
                        {"error": "Signature verification failed"},
                        "ERROR",
                        "INVALID_SIGNATURE"
                    )
                    raise ValueError("Signature verification failed")
        self.logger.log("reverse", signature, b"", {"error": "Signature not found"}, "ERROR", "NOT_FOUND")
        self.logger.execution_time_ns = time.time_ns() - start_time
        raise ValueError("Signature not found in log")

    def mimic_transformation(self, input_data: bytes, ref_input: bytes, ref_output: bytes) -> bytes:
        start_time = time.time_ns()
        adjusted_input = input_data
        if len(input_data) < len(ref_input):
            adjusted_input = input_data + b'\x00' * (len(ref_input) - len(input_data))
        elif len(input_data) > len(ref_input):
            adjusted_input = input_data[:len(ref_input)]
        output = self.compute(adjusted_input)
        self.logger.log(
            "mimic_ed25519",
            input_data,
            output,
            {"adjusted_input": adjusted_input.hex(), "ref_input": ref_input.hex(), "ref_output": ref_output.hex()},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return output

class TCCAES(TCCAlgorithm):
    def __init__(self, key: bytes):
        super().__init__()
        if len(key) != 16:
            raise ValueError("AES key must be 16 bytes")
        self.key = key
        self.sbox = [
            0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5, 0x30, 0x01, 0x67, 0x2b, 0xfe, 0xd7, 0xab, 0x76,
            0xca, 0x82, 0xc9, 0x7d, 0xfa, 0x59, 0x47, 0xf0, 0xad, 0xd4, 0xa2, 0xaf, 0x9c, 0xa4, 0x72, 0xc0,
            0xb7, 0xfd, 0x93, 0x26, 0x36, 0x3f, 0xf7, 0xcc, 0x34, 0xa5, 0xe5, 0xf1, 0x71, 0xd8, 0x31, 0x15,
            0x04, 0xc7, 0x23, 0xc3, 0x18, 0x96, 0x05, 0x9a, 0x07, 0x12, 0x80, 0xe2, 0xeb, 0x27, 0xb2, 0x75,
            0x09, 0x83, 0x2c, 0x1a, 0x1b, 0x6e, 0x5a, 0xa0, 0x52, 0x3b, 0xd6, 0xb3, 0x29, 0xe3, 0x2f, 0x84,
            0x53, 0xd1, 0x00, 0xed, 0x20, 0xfc, 0xb1, 0x5b, 0x6a, 0xcb, 0xbe, 0x39, 0x4a, 0x4c, 0x58, 0xcf,
            0xd0, 0xef, 0xaa, 0xfb, 0x43, 0x4d, 0x33, 0x85, 0x45, 0xf9, 0x02, 0x7f, 0x50, 0x3c, 0x9f, 0xa8,
            0x51, 0xa3, 0x40, 0x8f, 0x92, 0x9d, 0x38, 0xf5, 0xbc, 0xb6, 0xda, 0x21, 0x10, 0xff, 0xf3, 0xd2,
            0xcd, 0x0c, 0x13, 0xec, 0x5f, 0x97, 0x44, 0x17, 0xc4, 0xa7, 0x7e, 0x3d, 0x64, 0x5d, 0x19, 0x73,
            0x60, 0x81, 0x4f, 0xdc, 0x22, 0x2a, 0x90, 0x88, 0x46, 0xee, 0xb8, 0x14, 0xde, 0x5e, 0x0b, 0xdb,
            0xe0, 0x32, 0x3a, 0x0a, 0x49, 0x06, 0x24, 0x5c, 0xc2, 0xd3, 0xac, 0x62, 0x91, 0x95, 0xe4, 0x79,
            0xe7, 0xc8, 0x37, 0x6d, 0x8d, 0xd5, 0x4e, 0xa9, 0x6c, 0x56, 0xf4, 0xea, 0x65, 0x7a, 0xae, 0x08,
            0xba, 0x78, 0x25, 0x2e, 0x1c, 0xa6, 0xb4, 0xc6, 0xe8, 0xdd, 0x74, 0x1f, 0x4b, 0xbd, 0x8b, 0x8a,
            0x70, 0x3e, 0xb5, 0x66, 0x48, 0x03, 0xf6, 0x0e, 0x61, 0x35, 0x57, 0xb9, 0x86, 0xc1, 0x1d, 0x9e,
            0xe1, 0xf8, 0x98, 0x11, 0x69, 0xd9, 0x8e, 0x94, 0x9b, 0x1e, 0x87, 0xe9, 0xce, 0x55, 0x28, 0xdf
            
        ]
        self.inv_sbox = [
            0x52, 0x09, 0x6a, 0xd5, 0x30, 0x36, 0xa5, 0x38, 0xbf, 0x40, 0xa3, 0x9e, 0x81, 0xf3, 0xd7, 0xfb,
            0x7c, 0xe3, 0x39, 0x82, 0x9b, 0x2f, 0xff, 0x87, 0x34, 0x8e, 0x43, 0x44, 0xc4, 0xde, 0xe9, 0xcb,
            0x54, 0x7b, 0x94, 0x32, 0xa6, 0xc2, 0x23, 0x3d, 0xee, 0x4c, 0x95, 0x0b, 0x42, 0xfa, 0xc3, 0x4e,
            0x08, 0x2e, 0xa1, 0x66, 0x28, 0xd9, 0x24, 0xb2, 0x76, 0x5b, 0xa2, 0x49, 0x6d, 0x8b, 0xd1, 0x25,
            0x72, 0xf8, 0xf6, 0x64, 0x86, 0x68, 0x98, 0x16, 0xd4, 0xa4, 0x5c, 0xcc, 0x5d, 0x65, 0xb6, 0x92,
            0x6c, 0x70, 0x48, 0x50, 0xfd, 0xed, 0xb9, 0xda, 0x5e, 0x15, 0x46, 0x57, 0xa7, 0x8d, 0x9d, 0x84,
            0x90, 0xd8, 0xab, 0x00, 0x8c, 0xbc, 0xd3, 0x0a, 0xf7, 0xe4, 0x58, 0x05, 0xb8, 0xb3, 0x45, 0x06,
            0xd0, 0x2c, 0x1e, 0x8f, 0xca, 0x3f, 0x0f, 0x02, 0xc1, 0xaf, 0xbd, 0x03, 0x01, 0x13, 0x8a, 0x6b,
            0x3a, 0x91, 0x11, 0x41, 0x4f, 0x67, 0xdc, 0xea, 0x97, 0xf2, 0xcf, 0xce, 0xf0, 0xb4, 0xe6, 0x73,
            0x96, 0xac, 0x74, 0x22, 0xe7, 0xad, 0x35, 0x85, 0xe2, 0xf9, 0x37, 0xe8, 0x1c, 0x75, 0xdf, 0x6e,
            0x47, 0xf1, 0x1a, 0x71, 0x1d, 0x29, 0xc5, 0x89, 0x6f, 0xb7, 0x62, 0x0e, 0xaa, 0x18, 0xbe, 0x1b,
            0xfc, 0x56, 0x3e, 0x4b, 0xc6, 0xd2, 0x79, 0x20, 0x9a, 0xdb, 0xc0, 0xfe, 0x78, 0xcd, 0x5a, 0xf4,
            0x1f, 0xdd, 0xa8, 0x33, 0x88, 0x07, 0xc7, 0x31, 0xb1, 0x12, 0x10, 0x59, 0x27, 0x80, 0xec, 0x5f,
            0x60, 0x51, 0x7f, 0xa9, 0x19, 0xb5, 0x4a, 0x0d, 0x2d, 0xe5, 0x7a, 0x9f, 0x93, 0xc9, 0x9c, 0xef,
            0xa0, 0xe0, 0x3b, 0x4d, 0xae, 0x2a, 0xf5, 0xb0, 0xc8, 0xeb, 0xbb, 0x3c, 0x83, 0x53, 0x99, 0x61,
            0x17, 0x2b, 0x04, 0x7e, 0xba, 0x77, 0xd6, 0x26, 0xe1, 0x69, 0x14, 0x63, 0x55, 0x21, 0x0c, 0x7d
        ]
        self.round_keys = self._expand_key()

    def _expand_key(self) -> List[bytes]:
        start_time = time.time_ns()
        key = list(self.key)
        round_keys = [key[:]]
        rcon = [0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80, 0x1b, 0x36]
        
        for i in range(10):
            last = round_keys[-1]
            new_key = []
            temp = last[-4:]
            temp = [self.sbox[b] for b in temp[1:] + temp[:1]]
            temp[0] ^= rcon[i]
            for j in range(4):
                new_key.append(last[j] ^ temp[j])
            for j in range(4, 16):
                if j % 4 == 0:
                    temp = new_key[-4:]
                new_key.append(last[j] ^ temp[j % 4])
            
            round_keys.append(new_key)
            self.logger.log(
                "key_expand",
                bytes(last),
                bytes(new_key),
                {"round": i, "rcon": hex(rcon[i])},
                "INFO",
                "SUCCESS"
            )
        
        self.logger.execution_time_ns = time.time_ns() - start_time
        return [bytes(k) for k in round_keys]

    def _sub_bytes(self, state: bytes) -> bytes:
        start_time = time.time_ns()
        result = bytes(self.sbox[b] for b in state)
        self.logger.log(
            "sub_bytes",
            state,
            result,
            {},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return result

    def _inv_sub_bytes(self, state: bytes) -> bytes:
        start_time = time.time_ns()
        result = bytes(self.inv_sbox[b] for b in state)
        self.logger.log(
            "inv_sub_bytes",
            state,
            result,
            {},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return result

    def _shift_rows(self, state: bytes) -> bytes:
        start_time = time.time_ns()
        s = list(state)
        result = [
            s[0], s[5], s[10], s[15],
            s[4], s[9], s[14], s[3],
            s[8], s[13], s[2], s[7],
            s[12], s[1], s[6], s[11]
        ]
        result = bytes(result)
        self.logger.log(
            "shift_rows",
            state,
            result,
            {},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return result

    def _inv_shift_rows(self, state: bytes) -> bytes:
        start_time = time.time_ns()
        s = list(state)
        result = [
            s[0], s[13], s[10], s[7],
            s[4], s[1], s[14], s[11],
            s[8], s[5], s[2], s[15],
            s[12], s[9], s[6], s[3]
        ]
        result = bytes(result)
        self.logger.log(
            "inv_shift_rows",
            state,
            result,
            {},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return result

    def _mix_columns(self, state: bytes) -> bytes:
        start_time = time.time_ns()
        s = list(state)
        result = [0] * 16
        for c in range(4):
            result[c*4] = (self._gf_mult(2, s[c*4]) ^ self._gf_mult(3, s[c*4+1]) ^ s[c*4+2] ^ s[c*4+3]) & 0xFF
            result[c*4+1] = (s[c*4] ^ self._gf_mult(2, s[c*4+1]) ^ self._gf_mult(3, s[c*4+2]) ^ s[c*4+3]) & 0xFF
            result[c*4+2] = (s[c*4] ^ s[c*4+1] ^ self._gf_mult(2, s[c*4+2]) ^ self._gf_mult(3, s[c*4+3])) & 0xFF
            result[c*4+3] = (self._gf_mult(3, s[c*4]) ^ s[c*4+1] ^ s[c*4+2] ^ self._gf_mult(2, s[c*4+3])) & 0xFF
        result = bytes(result)
        self.logger.log(
            "mix_columns",
            state,
            result,
            {},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return result

    def _inv_mix_columns(self, state: bytes) -> bytes:
        start_time = time.time_ns()
        s = list(state)
        result = [0] * 16
        for c in range(4):
            result[c*4] = (self._gf_mult(0x0e, s[c*4]) ^ self._gf_mult(0x0b, s[c*4+1]) ^ self._gf_mult(0x0d, s[c*4+2]) ^ self._gf_mult(0x09, s[c*4+3])) & 0xFF
            result[c*4+1] = (self._gf_mult(0x09, s[c*4]) ^ self._gf_mult(0x0e, s[c*4+1]) ^ self._gf_mult(0x0b, s[c*4+2]) ^ self._gf_mult(0x0d, s[c*4+3])) & 0xFF
            result[c*4+2] = (self._gf_mult(0x0d, s[c*4]) ^ self._gf_mult(0x09, s[c*4+1]) ^ self._gf_mult(0x0e, s[c*4+2]) ^ self._gf_mult(0x0b, s[c*4+3])) & 0xFF
            result[c*4+3] = (self._gf_mult(0x0b, s[c*4]) ^ self._gf_mult(0x0d, s[c*4+1]) ^ self._gf_mult(0x09, s[c*4+2]) ^ self._gf_mult(0x0e, s[c*4+3])) & 0xFF
        result = bytes(result)
        self.logger.log(
            "inv_mix_columns",
            state,
            result,
            {},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return result

    def _gf_mult(self, a: int, b: int) -> int:
        p = 0
        for _ in range(8):
            if b & 1:
                p ^= a
            high_bit = a & 0x80
            a = (a << 1) & 0xFF
            if high_bit:
                a ^= 0x1b
            b >>= 1
        return p

    def _add_round_key(self, state: bytes, round_key: bytes) -> bytes:
        start_time = time.time_ns()
        result = bytes(a ^ b for a, b in zip(state, round_key))
        self.logger.log(
            "add_round_key",
            state + round_key,
            result,
            {},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return result

    def compute(self, plaintext: bytes) -> bytes:
        start_time = time.time_ns()
        if len(plaintext) != 16:
            self.logger.log("compute", plaintext, b"", {"error": "Plaintext must be 16 bytes"}, "ERROR", "INVALID_INPUT")
            raise ValueError("Plaintext must be 16 bytes")
        state = plaintext
        state = self._add_round_key(state, self.round_keys[0])
        for i in range(1, 10):
            state = self._sub_bytes(state)
            state = self._shift_rows(state)
            state = self._mix_columns(state)
            state = self._add_round_key(state, self.round_keys[i])
        state = self._sub_bytes(state)
        state = self._shift_rows(state)
        state = self._add_round_key(state, self.round_keys[10])
        self.logger.log(
            "finalize",
            plaintext,
            state,
            {"rounds": 10},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return state

    def reverse(self, ciphertext: bytes) -> bytes:
        start_time = time.time_ns()
        if len(ciphertext) != 16:
            self.logger.log("reverse", ciphertext, b"", {"error": "Ciphertext must be 16 bytes"}, "ERROR", "INVALID_INPUT")
            raise ValueError("Ciphertext must be 16 bytes")
        state = ciphertext
        state = self._add_round_key(state, self.round_keys[10])
        state = self._inv_shift_rows(state)
        state = self._inv_sub_bytes(state)
        for i in range(9, 0, -1):
            state = self._add_round_key(state, self.round_keys[i])
            state = self._inv_mix_columns(state)
            state = self._inv_shift_rows(state)
            state = self._inv_sub_bytes(state)
        state = self._add_round_key(state, self.round_keys[0])
        self.logger.log(
            "reverse_complete",
            ciphertext,
            state,
            {"reconstructed_length": len(state)},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return state

    def mimic_transformation(self, input_data: bytes, ref_input: bytes, ref_output: bytes) -> bytes:
        start_time = time.time_ns()
        adjusted_input = input_data
        if len(input_data) < 16:
            adjusted_input = input_data + b'\x00' * (16 - len(input_data))
        elif len(input_data) > 16:
            adjusted_input = input_data[:16]
        output = self.compute(adjusted_input)
        self.logger.log(
            "mimic_aes",
            input_data,
            output,
            {"adjusted_input": adjusted_input.hex(), "ref_input": ref_input.hex(), "ref_output": ref_output.hex()},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return output

class TCCKeccak(TCCAlgorithm):
    BLOCK_SIZE_BYTES = 136
    STATE_SIZE = 200
    RATE = 136
    CAPACITY = 64
    ROUND_CONSTANTS = [
        0x0000000000000001, 0x0000000000008082, 0x800000000000808a, 0x8000000080008000,
        0x000000000000808b, 0x0000000080000001, 0x8000000080008081, 0x8000000000008009,
        0x000000000000008a, 0x0000000000000088, 0x0000000080008009, 0x000000008000000a,
        0x000000008000808b, 0x800000000000008b, 0x8000000000008089, 0x8000000000008003,
        0x8000000000008002, 0x8000000000000080, 0x000000008000800a, 0x800000008000000a,
        0x8000000080008081, 0x8000000000008080, 0x0000000080000001, 0x8000000080008008
    ]

    def __init__(self):
        super().__init__()
        self.state = [0] * 25
        self.reset()

    def reset(self) -> None:
        self.state = [0] * 25
        self.logger.log("keccak_reset", b"", b"", {"state": "zeroed"}, "INFO", "SUCCESS")

    def _rotate_left(self, x: int, n: int) -> int:
        return ((x << n) | (x >> (64 - n))) & 0xFFFFFFFFFFFFFFFF

    def _theta(self) -> None:
        C = [0] * 5
        D = [0] * 5
        for x in range(5):
            C[x] = self.state[x] ^ self.state[x + 5] ^ self.state[x + 10] ^ self.state[x + 15] ^ self.state[x + 20]
        for x in range(5):
            D[x] = C[(x - 1) % 5] ^ self._rotate_left(C[(x + 1) % 5], 1)
        for x in range(5):
            for y in range(5):
                self.state[x + 5 * y] ^= D[x]

    def _rho_pi(self) -> None:
        temp = self.state[1]
        offsets = [
            (0, 1, 0), (6, 44, 6), (9, 20, 9), (22, 61, 22), (14, 39, 14),
            (20, 18, 20), (2, 62, 2), (12, 43, 12), (13, 25, 13), (19, 8, 19),
            (23, 56, 23), (15, 41, 15), (4, 27, 4), (24, 14, 24), (21, 2, 21),
            (8, 55, 8), (16, 45, 16), (5, 36, 5), (3, 28, 3), (18, 21, 18),
            (17, 15, 17), (11, 10, 11), (7, 6, 7), (10, 3, 10)
        ]
        for src_idx, rot, dest_idx in offsets[1:]:
            self.state[dest_idx] = self._rotate_left(self.state[src_idx], rot)
        self.state[10] = temp

    def _chi(self) -> None:
        for y in range(0, 25, 5):
            A = self.state[y:y+5].copy()
            for x in range(5):
                self.state[y + x] = A[x] ^ ((~A[(x + 1) % 5]) & A[(x + 2) % 5])

    def _iota(self, round: int) -> None:
        self.state[0] ^= self.ROUND_CONSTANTS[round]

    def _permutation(self) -> None:
        for round in range(24):
            before_hash = hashlib.sha256(bytearray([b & 0xFF for lane in self.state for b in lane.to_bytes(8, 'little')])).digest()
            self._theta()
            self._rho_pi()
            self._chi()
            self._iota(round)
            after_hash = hashlib.sha256(bytearray([b & 0xFF for lane in self.state for b in lane.to_bytes(8, 'little')])).digest()
            self.logger.log(
                "keccak_permutation",
                before_hash,
                after_hash,
                {"round": round},
                "INFO",
                "SUCCESS"
            )

    def _pad(self, data: bytes) -> bytes:
        start_time = time.time_ns()
        len_data = len(data)
        pad_len = self.RATE - (len_data % self.RATE)
        padded = data + b'\x01' + b'\x00' * (pad_len - 2) + b'\x80'
        self.logger.log(
            "keccak_pad",
            data,
            padded,
            {"original_length": len_data, "padded_length": len(padded)},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return padded

    def _absorb(self, chunk: bytes) -> None:
        start_time = time.time_ns()
        if len(chunk) != self.RATE:
            raise ValueError(f"Chunk must be {self.RATE} bytes")
        before_hash = hashlib.sha256(bytearray([b & 0xFF for lane in self.state for b in lane.to_bytes(8, 'little')])).digest()
        for i in range(self.RATE // 8):
            lane = int.from_bytes(chunk[i*8:(i+1)*8], 'big')
            self.state[i] ^= lane
        self._permutation()
        after_hash = hashlib.sha256(bytearray([b & 0xFF for lane in self.state for b in lane.to_bytes(8, 'little')])).digest()
        self.logger.log(
            "keccak_absorb",
            chunk,
            after_hash,
            {"state_hash_before": before_hash.hex()},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time

    def _squeeze(self) -> bytes:
        start_time = time.time_ns()
        result = b''
        for i in range(4):
            result += self.state[i].to_bytes(8, 'big')
        self.logger.log(
            "keccak_squeeze",
            b"",
            result,
            {"hash_length": len(result)},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return result

    def compute(self, input_data: bytes) -> bytes:
        start_time = time.time_ns()
        self.reset()
        padded = self._pad(input_data)
        for i in range(0, len(padded), self.RATE):
            chunk = padded[i:i+self.RATE]
            self._absorb(chunk)
        result = self._squeeze()
        self.logger.log(
            "keccak_finalize",
            input_data,
            result,
            {"execution_time_ns": time.time_ns() - start_time},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return result

    def reverse(self, target_output: bytes) -> bytes:
        start_time = time.time_ns()
        if len(target_output) != 32:
            self.logger.log("reverse", target_output, b"", {"error": "Invalid target hash"}, "ERROR", "INVALID_INPUT")
            raise ValueError("Target hash must be 32 bytes")
        for entry in reversed(self.logger.tcc_log):
            if entry.operation == "keccak_finalize" and entry.output_data == target_output:
                self.logger.log(
                    "reverse_complete",
                    target_output,
                    entry.input_data,
                    {"reconstructed_length": len(entry.input_data)},
                    "INFO",
                    "SUCCESS"
                )
                self.logger.execution_time_ns = time.time_ns() - start_time
                return entry.input_data
        self.logger.log("reverse", target_output, b"", {"error": "Target hash not found"}, "ERROR", "NOT_FOUND")
        self.logger.execution_time_ns = time.time_ns() - start_time
        raise ValueError("Target hash not found in log")

    def mimic_transformation(self, input_data: bytes, ref_input: bytes, ref_output: bytes) -> bytes:
        start_time = time.time_ns()
        adjusted_input = input_data
        if len(input_data) < len(ref_input):
            adjusted_input = input_data + b'\x00' * (len(ref_input) - len(input_data))
        elif len(input_data) > len(ref_input):
            adjusted_input = input_data[:len(ref_input)]
        output = self.compute(adjusted_input)
        self.logger.log(
            "mimic_keccak",
            input_data,
            output,
            {"adjusted_input": adjusted_input.hex(), "ref_input": ref_input.hex(), "ref_output": ref_output.hex()},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return output

class StorageShard:
    def __init__(self, owner: str):
        self.owner = owner
        self.data = b""
        self.logger = TCCLogger()

    def store_data(self, data: bytes) -> None:
        start_time = time.time_ns()
        self.data = data
        data_hash = hashlib.sha256(data).digest()
        self.logger.log(
            "store_data",
            data,
            data_hash,
            {"owner": self.owner},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time

    def get_data(self) -> bytes:
        start_time = time.time_ns()
        self.logger.log(
            "get_data",
            b"",
            self.data,
            {"owner": self.owner},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return self.data

    def save_log(self, filename: str) -> None:
        self.logger.save_log(filename)

class TCCKeccakEngine:
    STATE_SIZE = 200
    RATE = 136
    MAX_ITERATIONS = 100
    MAX_STEPS = 1000
    FEE_PER_ENTROPY = 1000

    def __init__(self, initial_fee: int = 1000, initial_max_steps: int = 1000):
        self.fee_per_entropy = initial_fee
        self.max_steps = initial_max_steps
        self.step_count = 0
        self.internal_state = bytearray(self.STATE_SIZE)
        self.commitments = {}
        self.commitment_timestamps = {}
        self.sponge_steps = {}
        self.shards = []
        self.logger = TCCLogger()
        self.keccak = TCCKeccak()

    def commit_entropy(self, user_id: str, commitment: bytes) -> None:
        start_time = time.time_ns()
        if len(commitment) != 32:
            self.logger.log("commit_entropy", commitment, b"", {"error": "Commitment must be 32 bytes"}, "ERROR", "INVALID_INPUT")
            raise ValueError("Commitment must be 32 bytes")
        if user_id in self.commitments:
            self.logger.log("commit_entropy", commitment, b"", {"error": "Commitment already exists for user"}, "ERROR", "DUPLICATE_COMMITMENT")
            raise ValueError("Commitment already exists for user")
        self.commitments[user_id] = commitment
        self.commitment_timestamps[user_id] = time.time_ns()
        self.logger.log(
            "entropy_committed",
            commitment,
            b"",
            {"user_id": user_id, "commitment_hash": commitment.hex()},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time

    def batch_commit_entropy(self, user_id: str, commitments: List[bytes]) -> None:
        start_time = time.time_ns()
        if len(commitments) > 50:
            self.logger.log("batch_commit_entropy", b"", b"", {"error": "Too many commitments"}, "ERROR", "INVALID_INPUT")
            raise ValueError("Too many commitments")
        for commitment in commitments:
            self.commit_entropy(user_id, commitment)
        self.logger.log(
            "batch_committed",
            b"",
            b"",
            {"user_id": user_id, "commitment_count": len(commitments)},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time

    def reveal_entropy(self, user_id: str, entropy: bytes, fee: int) -> None:
        start_time = time.time_ns()
        if fee < self.fee_per_entropy:
            self.logger.log("reveal_entropy", entropy, b"", {"error": "Insufficient fee"}, "ERROR", "INSUFFICIENT_FEE")
            raise ValueError("Insufficient fee")
        if user_id not in self.commitments:
            self.logger.log("reveal_entropy", entropy, b"", {"error": "No commitment found"}, "ERROR", "NO_COMMITMENT")
            raise ValueError("No commitment found")
        commitment = hashlib.sha256(entropy).digest()
        if self.commitments[user_id] != commitment:
            self.logger.log("reveal_entropy", entropy, b"", {"error": "Invalid commitment"}, "ERROR", "INVALID_COMMITMENT")
            raise ValueError("Invalid commitment")
        if (time.time_ns() - self.commitment_timestamps[user_id]) > 86400 * 1_000_000_000:
            self.logger.log("reveal_entropy", entropy, b"", {"error": "Commitment expired"}, "ERROR", "COMMITMENT_EXPIRED")
            raise ValueError("Commitment expired")
        del self.commitments[user_id]
        del self.commitment_timestamps[user_id]
        entropy_hash = hashlib.sha256(entropy).digest()
        self.logger.log(
            "entropy_revealed",
            entropy,
            entropy_hash,
            {"user_id": user_id},
            "INFO",
            "SUCCESS"
        )
        self._feed_entropy(entropy)
        self.logger.execution_time_ns = time.time_ns() - start_time

    def batch_reveal_entropy(self, user_id: str, entropies: List[bytes], fee: int) -> None:
        start_time = time.time_ns()
        required_fee = self.fee_per_entropy * len(entropies)
        if fee < required_fee:
            self.logger.log("batch_reveal_entropy", b"", b"", {"error": "Insufficient fee"}, "ERROR", "INSUFFICIENT_FEE")
            raise ValueError("Insufficient fee")
        for entropy in entropies:
            self.reveal_entropy(user_id, entropy, self.fee_per_entropy)
        self.logger.log(
            "batch_revealed",
            b"",
            b"",
            {"user_id": user_id, "entropy_count": len(entropies)},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time

    def _feed_entropy(self, input_data: bytes) -> None:
        start_time = time.time_ns()
        if self.step_count >= self.max_steps:
            self.logger.log("feed_entropy", input_data, b"", {"error": "Max steps reached"}, "ERROR", "MAX_STEPS")
            raise ValueError("Max steps reached")
        index = 0
        iterations = 0
        while index < len(input_data) and iterations < self.MAX_ITERATIONS:
            chunk = input_data[index:index+self.RATE]
            padded_chunk = self.keccak._pad(chunk)
            before_absorb_hash = hashlib.sha256(self.internal_state).digest()
            self.keccak._absorb(padded_chunk)
            after_permute_hash = self.keccak._squeeze()
            self.sponge_steps[self.step_count] = {
                "input_chunk_hash": hashlib.sha256(chunk).digest(),
                "before_absorb_hash": before_absorb_hash,
                "after_permute_hash": after_permute_hash
            }
            self.logger.log(
                "sponge_step",
                chunk,
                after_permute_hash,
                {
                    "step_id": self.step_count,
                    "input_chunk_hash": hashlib.sha256(chunk).hexdigest(),
                    "before_absorb_hash": before_absorb_hash.hex(),
                    "after_permute_hash": after_permute_hash.hex()
                },
                "INFO",
                "SUCCESS"
            )
            if self.shards:
                self.shards[-1].store_data(chunk)
            self.internal_state = bytearray([b & 0xFF for lane in self.keccak.state for b in lane.to_bytes(8, 'little')])[:self.STATE_SIZE]
            self.step_count += 1
            index += self.RATE
            iterations += 1
        if iterations >= self.MAX_ITERATIONS:
            self.logger.log("feed_entropy", input_data, b"", {"error": "Max iterations reached"}, "ERROR", "MAX_ITERATIONS")
            raise ValueError("Max iterations reached")
        self.logger.execution_time_ns = time.time_ns() - start_time

    def deploy_shard(self, owner: str) -> StorageShard:
        start_time = time.time_ns()
        shard = StorageShard(owner)
        self.shards.append(shard)
        self.logger.log(
            "shard_deployed",
            b"",
            b"",
            {"owner": owner, "shard_id": len(self.shards)-1},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return shard

    def get_state_hash(self) -> bytes:
        start_time = time.time_ns()
        state_hash = hashlib.sha256(self.internal_state).digest()
        self.logger.log(
            "get_state_hash",
            b"",
            state_hash,
            {},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return state_hash

    def get_step(self, step_id: int) -> Dict[str, bytes]:
        start_time = time.time_ns()
        if step_id not in self.sponge_steps:
            self.logger.log("get_step", b"", b"", {"error": "Step not found", "step_id": step_id}, "ERROR", "STEP_NOT_FOUND")
            raise ValueError("Step not found")
        step = self.sponge_steps[step_id]
        self.logger.log(
            "get_step",
            b"",
            b"",
            {"step_id": step_id, "input_chunk_hash": step["input_chunk_hash"].hex()},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return step

    def save_log(self, filename: str) -> None:
        self.logger.save_log(filename)

class EntropyCoordinator:
    def __init__(self, engine_a: TCCKeccakEngine, engine_b: TCCKeccakEngine, engine_c: TCCKeccakEngine):
        if not all([engine_a, engine_b, engine_c]) or len(set([id(engine_a), id(engine_b), id(engine_c)])) != 3:
            raise ValueError("Invalid or duplicate engines")
        self.engine_a = engine_a
        self.engine_b = engine_b
        self.engine_c = engine_c
        self.logger = TCCLogger()

    def commit_entropy_all(self, user_id: str, commitment_a: bytes, commitment_b: bytes, commitment_c: bytes) -> None:
        start_time = time.time_ns()
        self.engine_a.commit_entropy(user_id, commitment_a)
        self.engine_b.commit_entropy(user_id, commitment_b)
        self.engine_c.commit_entropy(user_id, commitment_c)
        self.logger.log(
            "commit_entropy_all",
            commitment_a + commitment_b + commitment_c,
            b"",
            {"user_id": user_id},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time

    def batch_commit_entropy_all(self, user_id: str, commitments_a: List[bytes], commitments_b: List[bytes], commitments_c: List[bytes]) -> None:
        start_time = time.time_ns()
        self.engine_a.batch_commit_entropy(user_id, commitments_a)
        self.engine_b.batch_commit_entropy(user_id, commitments_b)
        self.engine_c.batch_commit_entropy(user_id, commitments_c)
        self.logger.log(
            "batch_commit_entropy_all",
            b"",
            b"",
            {"user_id": user_id, "commitment_counts": [len(commitments_a), len(commitments_b), len(commitments_c)]},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time

    def reveal_entropy_all(self, user_id: str, entropy_a: bytes, entropy_b: bytes, entropy_c: bytes, fee: int) -> None:
        start_time = time.time_ns()
        total_fee = 0
        has_a = user_id in self.engine_a.commitments
        has_b = user_id in self.engine_b.commitments
        has_c = user_id in self.engine_c.commitments
        if has_a:
            total_fee += self.engine_a.fee_per_entropy
        if has_b:
            total_fee += self.engine_b.fee_per_entropy
        if has_c:
            total_fee += self.engine_c.fee_per_entropy
        if fee < total_fee:
            self.logger.log("reveal_entropy_all", entropy_a + entropy_b + entropy_c, b"", {"error": "Insufficient fee"}, "ERROR", "INSUFFICIENT_FEE")
            raise ValueError("Insufficient fee")
        if has_a:
            self.engine_a.reveal_entropy(user_id, entropy_a, self.engine_a.fee_per_entropy)
        if has_b:
            self.engine_b.reveal_entropy(user_id, entropy_b, self.engine_b.fee_per_entropy)
        if has_c:
            self.engine_c.reveal_entropy(user_id, entropy_c, self.engine_c.fee_per_entropy)
        self.logger.log(
            "reveal_entropy_all",
            entropy_a + entropy_b + entropy_c,
            b"",
            {"user_id": user_id, "total_fee": total_fee},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time

    def batch_reveal_entropy_all(self, user_id: str, entropies_a: List[bytes], entropies_b: List[bytes], entropies_c: List[bytes], fee: int) -> None:
        start_time = time.time_ns()
        total_fee = 0
        if user_id in self.engine_a.commitments:
            total_fee += self.engine_a.fee_per_entropy * len(entropies_a)
        if user_id in self.engine_b.commitments:
            total_fee += self.engine_b.fee_per_entropy * len(entropies_b)
        if user_id in self.engine_c.commitments:
            total_fee += self.engine_c.fee_per_entropy * len(entropies_c)
        if fee < total_fee:
            self.logger.log("batch_reveal_entropy_all", b"", b"", {"error": "Insufficient fee"}, "ERROR", "INSUFFICIENT_FEE")
            raise ValueError("Insufficient fee")
        if user_id in self.engine_a.commitments:
            self.engine_a.batch_reveal_entropy(user_id, entropies_a, self.engine_a.fee_per_entropy * len(entropies_a))
        if user_id in self.engine_b.commitments:
            self.engine_b.batch_reveal_entropy(user_id, entropies_b, self.engine_b.fee_per_entropy * len(entropies_b))
        if user_id in self.engine_c.commitments:
            self.engine_c.batch_reveal_entropy(user_id, entropies_c, self.engine_c.fee_per_entropy * len(entropies_c))
        self.logger.log(
            "batch_reveal_entropy_all",
            b"",
            b"",
            {"user_id": user_id, "entropy_counts": [len(entropies_a), len(entropies_b), len(entropies_c)]},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time

    def get_combined_entropy(self) -> bytes:
        start_time = time.time_ns()
        data = b""
        data += self.engine_a.get_state_hash()
        data += self.engine_b.get_state_hash()
        data += self.engine_c.get_state_hash()
        combined = hashlib.sha256(data).digest()
        self.logger.log(
            "get_combined_entropy",
            b"",
            combined,
            {},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return combined

    def save_log(self, filename: str) -> None:
        self.logger.save_log(filename)

class TCCFlow:
    def __init__(self, steps: List[Tuple[str, TCCAlgorithm, Dict[str, Any]]], reference_input: bytes):
        self.steps = steps
        self.logger = TCCLogger()
        self.flow_log: List[Dict[str, Any]] = []
        self.reference_input = reference [reference_input]
        self.reference_outputs = self._compute_reference_outputs()

    def _compute_reference_outputs(self) -> List[bytes]:
        outputs = [self.reference_input]
        current_data = self.reference_input
        for step_name, algo, _ in self.steps:
            output_data = algo.compute(current_data)
            outputs.append(output_data)
            current_data = output_data
        return outputs

    def execute(self, input_data: bytes) -> bytes:
        start_time = time.time_ns()
        if not isinstance(input_data, bytes):
            self.logger.log("execute", input_data, b"", {"error": "Input data must be bytes"}, "ERROR", "INVALID_INPUT")
            raise ValueError("Input data must be bytes")
        current_data = input_data
        for step_idx, (step_name, algo, params) in enumerate(self.steps):
            if step_name == "encrypt_aes" and len(current_data) != 16:
                current_data = (current_data + b'\x00' * (16 - len(current_data)))[:16]
            output_data = algo.compute(current_data)
            metadata = {
                "step_index": step_idx,
                "step_name": step_name,
                "params": params,
                "input_length": len(current_data),
                "output_length": len(output_data)
            }
            self.logger.log(
                f"flow_{step_name}",
                current_data,
                output_data,
                metadata,
                "INFO",
                "SUCCESS"
            )
            self.flow_log.append({
                "step_index": step_idx,
                "step_name": step_name,
                "operation_id": self.logger.tcc_log[-1].operation_id,
                "input_data": base64.b64encode(current_data).decode('utf-8'),
                "output_data": base64.b64encode(output_data).decode('utf-8'),
                "timestamp": time.time_ns()
            })
            current_data = output_data
        self.logger.log(
            "flow_complete",
            input_data,
            current_data,
            {"total_steps": len(self.steps), "execution_time_ns": time.time_ns() - start_time},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return current_data

    def reverse(self, target_output: bytes) -> bytes:
        start_time = time.time_ns()
        for entry in reversed(self.logger.tcc_log):
            if entry.operation == "flow_complete" and entry.output_data == target_output:
                current_output = target_output
                for step_idx in range(len(self.steps) - 1, -1, -1):
                    step_name, algo, _ = self.steps[step_idx]
                    current_input = algo.reverse(current_output)
                    self.logger.log(
                        f"reverse_{step_name}",
                        current_output,
                        current_input,
                        {"step_index": step_idx},
                        "INFO",
                        "SUCCESS"
                    )
                    current_output = current_input
                self.logger.log(
                    "reverse_complete",
                    target_output,
                    current_output,
                    {"reconstructed_length": len(current_output)},
                    "INFO",
                    "SUCCESS"
                )
                self.logger.execution_time_ns = time.time_ns() - start_time
                return current_output
        self.logger.log(
            "reverse",
            target_output,
            b"",
            {"error": "Target output not found"},
            "ERROR",
            "NOT_FOUND"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        raise ValueError("Target output not found in log")

    def reverse_arbitrary(self, target_output: bytes, arbitrary_input: bytes) -> bytes:
        start_time = time.time_ns()
        current_data = arbitrary_input
        for step_idx in range(len(self.steps)):
            step_name, algo, _ = self.steps[step_idx]
            ref_input = self.reference_outputs[step_idx]
            ref_output = self.reference_outputs[step_idx + 1]
            current_data = algo.mimic_transformation(current_data, ref_input, ref_output)
        current_output = target_output
        for step_idx in range(len(self.steps) - 1, -1, -1):
            step_name, algo, _ = self.steps[step_idx]
            current_input = algo.reverse(current_output)
            self.logger.log(
                f"reverse_arbitrary_{step_name}",
                current_output,
                current_input,
                {"step_index": step_idx, "arbitrary_input": arbitrary_input.hex()},
                "INFO",
                "SUCCESS"
            )
            current_output = current_input
        self.logger.log(
            "reverse_arbitrary_complete",
            target_output,
            current_output,
            {"reconstructed_length": len(current_output), "arbitrary_input": arbitrary_input.hex()},
            "INFO",
            "SUCCESS"
        )
        self.logger.execution_time_ns = time.time_ns() - start_time
        return current_output

    def save_flow_log(self, filename: str) -> None:
        with open(filename, 'w') as f:
            for entry in self.flow_log:
                f.write(json.dumps(entry) + '\n')

def define_default_flow(aes_key: bytes, ed25519_key: bytes, include_keccak: bool = False) -> TCCFlow:
    reference_input = b"baseline_input_for_state_changes"
    steps = [
        ("hash_sha256", TCCSHA256(), {}),
        ("sign_ed25519", TCCEd25519(ed25519_key), {}),
        ("encrypt_aes", TCCAES(aes_key), {})
    ]
    if include_keccak:
        steps.insert(1, ("hash_keccak", TCCKeccak(), {}))
    return TCCFlow(steps, reference_input)

def main():
    parser = argparse.ArgumentParser(description="TCC Flow Demonstrator with Entropy Engine")
    parser.add_argument("--input", type=str, default="test", help="Input data (string)")
    parser.add_argument("--arbitrary-input", type=str, help="Arbitrary input for reverse (string)")
    parser.add_argument("--target-output", type=str, help="Target output to reverse (hex)")
    parser.add_argument("--aes-key", type=str, required=True, help="AES key (32 hex chars)")
    parser.add_argument("--ed25519-key", type=str, required=True, help="Ed25519 private key (64 hex chars)")
    parser.add_argument("--log-file", type=str, default="tcc_flow_log.jsonl", help="Log file path")
    parser.add_argument("--include-keccak", action="store_true", help="Include Keccak in flow")
    parser.add_argument("--commit-entropy", type=str, help="Commit entropy (hex)")
    parser.add_argument("--reveal-entropy", type=str, help="Reveal entropy (hex)")
    parser.add_argument("--user-id", type=str, default="user1", help="User ID for entropy operations")
    parser.add_argument("--fee", type=int, default=1000, help="Fee for entropy reveal")
    parser.add_argument("--deploy-shard", action="store_true", help="Deploy a storage shard")
    args = parser.parse_args()

    # Validate log-file
    if not re.match(r'^[a-zA-Z0-9_-.]+$', args.log_file):
        parser.error("Invalid --log-file: must contain only ASCII letters, numbers, underscores, hyphens, or dots")

    input_data = args.input.encode('utf-8')
    try:
        aes_key = bytes.fromhex(args.aes_key)
        ed25519_key = bytes.fromhex(args.ed25519_key)
    except ValueError as e:
        print(f"Invalid key format: {str(e)}")
        return

    if len(aes_key) != 16:
        print("AES key must be 16 bytes (32 hex chars)")
        return
    if len(ed25519_key) != 32:
        print("Ed25519 private key must be 32 bytes (64 hex chars)")
        return

    engine_a = TCCKeccakEngine()
    engine_b = TCCKeccakEngine()
    engine_c = TCCKeccakEngine()
    coordinator = EntropyCoordinator(engine_a, engine_b, engine_c)

    if args.commit_entropy:
        try:
            commitment = bytes.fromhex(args.commit_entropy)
            coordinator.commit_entropy_all(args.user_id, commitment, commitment, commitment)
            print(f"Committed entropy for user {args.user_id}: {commitment.hex()}")
        except ValueError as e:
            print(f"Entropy commit error: {str(e)}")
        return

    if args.reveal_entropy:
        try:
            entropy = bytes.fromhex(args.reveal_entropy)
            coordinator.reveal_entropy_all(args.user_id, entropy, entropy, entropy, args.fee)
            print(f"Revealed entropy for user {args.user_id}: {entropy.hex()}")
            combined_entropy = coordinator.get_combined_entropy()
            print(f"Combined entropy: {combined_entropy.hex()}")
        except ValueError as e:
            print(f"Entropy reveal error: {str(e)}")
        return

    if args.deploy_shard:
        shard = engine_a.deploy_shard(args.user_id)
        print(f"Deployed shard for {args.user_id}, shard ID: {len(engine_a.shards)-1}")
        return

    flow = define_default_flow(aes_key, ed25519_key, args.include_keccak)
    try:
        result = flow.execute(input_data)
        flow.save_flow_log(args.log_file)
        print(f"Input: {input_data.hex()}")
        print(f"Output: {result.hex()}")

        reconstructed = flow.reverse(result)
        print(f"Reconstructed input: {reconstructed.hex()}")
        print(f"Reverse successful: {reconstructed == input_data}")

        if args.arbitrary_input and args.target_output:
            try:
                arbitrary_input = args.arbitrary_input.encode('utf-8')
                target_output = bytes.fromhex(args.target_output)
                reconstructed_arbitrary = flow.reverse_arbitrary(target_output, arbitrary_input)
                print(f"Arbitrary input: {arbitrary_input.hex()}")
                print(f"Target output: {target_output.hex()}")
                print(f"Reconstructed arbitrary input: {reconstructed_arbitrary.hex()}")
            except ValueError as e:
                print(f"Reverse arbitrary error: {str(e)}")
                return

        # Output structured result for FastAPI
        output = {
            "input": input_data.hex(),
            "output": result.hex(),
            "reconstructed_input": reconstructed.hex(),
            "reverse_successful": reconstructed == input_data,
            "log_file": args.log_file
        }
        if args.arbitrary_input and args.target_output:
            output["arbitrary_input"] = arbitrary_input.hex()
            output["target_output"] = target_output.hex()
            output["reconstructed_arbitrary"] = reconstructed_arbitrary.hex()
        print(json.dumps(output))
    except Exception as e:
        print(json.dumps({"error": str(e)}))

if __name__ == "__main__":
    main()

import numpy as np
from scipy import signal
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict, Any
from sklearn.ensemble import IsolationForest
import time
import json
import hashlib
import base64
import struct
import pandas as pd
import os
import logging
from functools import lru_cache
from pathlib import Path

# Configure logging with configurable levels
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TCCLogger:
    """Transaction Chain Code Logger for auditable, hash-linked logs."""
    def __init__(self, log_level: str = "INFO"):
        self.tcc_log: List['TCCLogEntry'] = []
        self.step_counter: int = 0
        self.log_level: int = getattr(logging, log_level.upper(), logging.INFO)

    def log(self, operation: str, input_data: bytes, output_data: bytes,
            metadata: Optional[Dict[str, Any]] = None,
            log_level: str = "INFO", error_code: str = "NONE") -> None:
        if getattr(logging, log_level.upper(), logging.INFO) >= self.log_level:
            try:
                entry = TCCLogEntry(
                    self.step_counter, operation, input_data, output_data,
                    metadata or {}, log_level, error_code, prev_hash=self._compute_prev_hash()
                )
                self.tcc_log.append(entry)
                self.step_counter += 1
                logger.log(self.log_level, f"Logged operation: {operation}, step: {self.step_counter}")
            except Exception as e:
                logger.error(f"Logging failed for operation {operation}: {str(e)}")

    def _compute_prev_hash(self) -> bytes:
        if not self.tcc_log:
            return b'\x00' * 32
        last_entry = self.tcc_log[-1]
        return hashlib.sha256(last_entry.to_bytes()).digest()

    def save_log(self, filename: str) -> None:
        try:
            with open(filename, 'w') as f:
                for entry in self.tcc_log:
                    f.write(json.dumps(entry.to_json()) + '\n')
            logger.info(f"Log saved to {filename}")
        except IOError as e:
            logger.error(f"Failed to save log to {filename}: {str(e)}")
            raise

class TCCLogEntry:
    """A single log entry with hash-linking for integrity."""
    def __init__(self, step: int, operation: str, input_data: bytes, output_data: bytes,
                 metadata: Dict[str, Any], log_level: str, error_code: str, prev_hash: bytes):
        self.step: int = step
        self.operation: str = operation
        self.input_data: bytes = input_data
        self.output_data: bytes = output_data
        self.metadata: Dict[str, Any] = metadata
        self.log_level: str = log_level
        self.error_code: str = error_code
        self.prev_hash: bytes = prev_hash
        self.operation_id: str = hashlib.sha256(f"{step}:{operation}:{time.time_ns()}".encode()).hexdigest()[:32]
        self.timestamp: int = time.time_ns()
        self.execution_time_ns: int = 0

    def to_bytes(self) -> bytes:
        start_time = time.time_ns()
        try:
            step_bytes = struct.pack('>I', self.step)
            op_bytes = self.operation.encode('utf-8').ljust(32, b'\x00')[:32]
            input_len_bytes = struct.pack('>I', len(self.input_data))
            output_len_bytes = struct.pack('>I', len(self.output_data))
            meta_bytes = json.dumps(self.metadata).encode('utf-8').ljust(128, b'\x00')[:128]
            level_bytes = self.log_level.encode('utf-8').ljust(16, b'\x00')[:16]
            error_bytes = self.error_code.encode('utf-8').ljust(16, b'\x00')[:16]
            op_id_bytes = self.operation_id.encode('utf-8').ljust(32, b'\x00')[:32]
            ts_bytes = struct.pack('>Q', self.timestamp)
            exec_time_bytes = struct.pack('>Q', self.execution_time_ns)
            result = (
                step_bytes + op_bytes + input_len_bytes + self.input_data +
                output_len_bytes + self.output_data + meta_bytes + level_bytes +
                error_bytes + self.prev_hash + op_id_bytes + ts_bytes + exec_time_bytes
            )
            self.execution_time_ns = time.time_ns() - start_time
            return result
        except Exception as e:
            logger.error(f"Failed to serialize log entry: {str(e)}")
            raise

    def to_json(self) -> Dict[str, Any]:
        return {
            "step": self.step,
            "operation": self.operation,
            "input_data": base64.b64encode(self.input_data).decode('utf-8'),
            "output_data": base64.b64encode(self.output_data).decode('utf-8'),
            "metadata": self.metadata,
            "log_level": self.log_level,
            "error_code": self.error_code,
            "prev_hash": base64.b64encode(self.prev_hash).decode('utf-8'),
            "operation_id": self.operation_id,
            "timestamp": self.timestamp,
            "execution_time_ns": self.execution_time_ns
        }

class BioInspiredSecuritySystem:
    """Bio-inspired anomaly detection system with FFT, Isolation Forest, adaptive thresholding, and robust audit logging."""
    def __init__(self, sample_rate: int = 1000, window_size: int = 100,
                 base_threshold: float = 0.1, adaptive_threshold_factor: float = 1.5,
                 log_level: str = "INFO"):
        self.sample_rate: int = sample_rate
        self.window_size: int = window_size
        self.base_threshold: float = base_threshold
        self.adaptive_threshold: float = base_threshold
        self.adaptive_threshold_factor: float = adaptive_threshold_factor
        self.execution_data: np.ndarray = np.array([])
        self.time_points: np.ndarray = np.array([])
        self.baseline_frequencies: Optional[np.ndarray] = None
        self.isolation_forest: IsolationForest = IsolationForest(contamination=0.1, random_state=42)
        self.anomaly_history: List[float] = []
        self.logger: TCCLogger = TCCLogger(log_level=log_level)
        self.start_time: float = time.time()

    def _generate_signal(self, is_malicious: bool = False, seed: Optional[int] = None) -> np.ndarray:
        """Generate synthetic time-domain signal with optional anomaly."""
        start_time = time.time_ns()
        try:
            if seed is not None:
                np.random.seed(seed)
            t = np.linspace(0, self.window_size / self.sample_rate, self.window_size)
            base_signal = (
                np.sin(2 * np.pi * 5 * t) +
                0.5 * np.sin(2 * np.pi * 10 * t) +
                0.2 * np.sin(2 * np.pi * 20 * t)
            )
            if is_malicious:
                anomaly = 0.8 * np.sin(2 * np.pi * np.random.uniform(40, 60) * t) * np.random.normal(1, 0.3, self.window_size)
                signal_out = base_signal + anomaly
            else:
                signal_out = base_signal + np.random.normal(0, 0.15, self.window_size)
            self.execution_data = signal_out
            self.time_points = t
            self.logger.log(
                "generate_signal", b"", signal_out.tobytes(),
                {"is_malicious": is_malicious, "seed": seed, "execution_time_ns": time.time_ns() - start_time}
            )
            return signal_out
        except Exception as e:
            self.logger.log(
                "generate_signal", b"", b"",
                {"error": str(e), "execution_time_ns": time.time_ns() - start_time}, "ERROR", "SIGNAL_GENERATION_FAILED"
            )
            raise

    @lru_cache(maxsize=32)
    def _compute_fft(self, data: tuple) -> Tuple[np.ndarray, np.ndarray]:
        """Compute FFT with windowing for frequency analysis, cached for performance."""
        start_time = time.time_ns()
        try:
            window = signal.windows.hann(self.window_size)
            fft_result = np.fft.fft(np.array(data) * window)
            freqs = np.fft.fftfreq(self.window_size, d=1 / self.sample_rate)
            magnitudes = np.abs(fft_result)
            mask = freqs > 0
            self.logger.log(
                "analyze_frequency", np.array(data).tobytes(), magnitudes[mask].tobytes(),
                {"freq_count": len(freqs[mask]), "execution_time_ns": time.time_ns() - start_time}
            )
            return freqs[mask], magnitudes[mask]
        except Exception as e:
            self.logger.log(
                "analyze_frequency", b"", b"",
                {"error": str(e), "execution_time_ns": time.time_ns() - start_time}, "ERROR", "FFT_COMPUTATION_FAILED"
            )
            raise

    def analyze_frequency_signature(self) -> Tuple[np.ndarray, np.ndarray]:
        """Wrapper for cached FFT computation."""
        return self._compute_fft(tuple(self.execution_data))

    def establish_baseline(self, num_samples: int = 10) -> np.ndarray:
        """Establish baseline using multiple samples and train Isolation Forest."""
        start_time = time.time_ns()
        try:
            all_magnitudes = []
            for i in range(num_samples):
                self._generate_signal(is_malicious=False, seed=i)
                _, magnitudes = self.analyze_frequency_signature()
                all_magnitudes.append(magnitudes)
            self.baseline_frequencies = np.mean(all_magnitudes, axis=0)
            self.isolation_forest.fit(np.array(all_magnitudes))
            self.logger.log(
                "establish_baseline", b"", self.baseline_frequencies.tobytes(),
                {"num_samples": num_samples, "execution_time_ns": time.time_ns() - start_time}
            )
            return self.baseline_frequencies
        except Exception as e:
            self.logger.log(
                "establish_baseline", b"", b"",
                {"error": str(e), "execution_time_ns": time.time_ns() - start_time}, "ERROR", "BASELINE_FAILED"
            )
            raise

    def _explain_anomaly(self, anomaly_score: float, freqs: np.ndarray, magnitudes: np.ndarray, baseline: np.ndarray) -> str:
        """Rule-based explanation for detected anomalies."""
        try:
            dominant_idx = np.argmax(np.abs(magnitudes - baseline))
            dominant_freq = freqs[dominant_idx]
            if anomaly_score > 2.0:
                return f"Severe anomaly: Unusual energy at {dominant_freq:.2f} Hz. Possible attack or malfunction detected."
            elif anomaly_score > 1.0:
                return f"Moderate anomaly: Elevated activity at {dominant_freq:.2f} Hz. Potential security issue or environmental change."
            elif anomaly_score > 0.5:
                return f"Mild anomaly: Slight deviation at {dominant_freq:.2f} Hz. Monitor for further irregularities."
            return "No significant anomaly detected."
        except Exception as e:
            logger.error(f"Anomaly explanation failed: {str(e)}")
            return "Anomaly explanation unavailable due to processing error."

    def detect_anomaly(self) -> Tuple[bool, float, str]:
        """Detect anomalies and provide explanation."""
        if self.baseline_frequencies is None:
            self.logger.log(
                "detect_anomaly", b"", b"",
                {"error": "Baseline not established"}, "ERROR", "NO_BASELINE"
            )
            raise ValueError("Baseline not established. Run establish_baseline() first.")

        start_time = time.time_ns()
        try:
            freqs, current_magnitudes = self.analyze_frequency_signature()
            deviation = np.abs(current_magnitudes - self.baseline_frequencies)
            fft_score = np.mean(deviation / (self.baseline_frequencies + 1e-8))
            ml_score = -self.isolation_forest.score_samples([current_magnitudes])[0]
            combined_score = 0.7 * fft_score + 0.3 * ml_score

            if self.anomaly_history:
                self.adaptive_threshold = np.mean(self.anomaly_history[-10:]) * self.adaptive_threshold_factor
            self.anomaly_history.append(combined_score)

            is_anomaly = combined_score > max(self.base_threshold, self.adaptive_threshold)
            explanation = self._explain_anomaly(combined_score, freqs, current_magnitudes, self.baseline_frequencies) if is_anomaly else ""

            self.logger.log(
                "detect_anomaly", current_magnitudes.tobytes(), np.array([is_anomaly, combined_score]).tobytes(),
                {
                    "anomaly_score": combined_score,
                    "threshold": self.adaptive_threshold,
                    "is_anomaly": is_anomaly,
                    "explanation": explanation,
                    "execution_time_ns": time.time_ns() - start_time
                }
            )
            return is_anomaly, combined_score, explanation
        except Exception as e:
            self.logger.log(
                "detect_anomaly", b"", b"",
                {"error": str(e), "execution_time_ns": time.time_ns() - start_time}, "ERROR", "ANOMALY_DETECTION_FAILED"
            )
            raise

    def apply_interference(self) -> np.ndarray:
        """Apply adaptive interference to suppress dominant anomaly frequencies."""
        start_time = time.time_ns()
        try:
            freqs, magnitudes = self.analyze_frequency_signature()
            dominant_freq = freqs[np.argmax(magnitudes)]
            interference_amplitude = 0.8 * np.max(magnitudes) / (np.max(self.baseline_frequencies + 1e-8))
            interference = -interference_amplitude * np.sin(2 * np.pi * dominant_freq * self.time_points + np.pi)
            self.execution_data += interference
            self.logger.log(
                "apply_interference", self.execution_data.tobytes(), interference.tobytes(),
                {
                    "dominant_freq": dominant_freq,
                    "amplitude": interference_amplitude,
                    "execution_time_ns": time.time_ns() - start_time
                }
            )
            return interference
        except Exception as e:
            self.logger.log(
                "apply_interference", b"", b"",
                {"error": str(e), "execution_time_ns": time.time_ns() - start_time}, "ERROR", "INTERFERENCE_FAILED"
            )
            raise

    def real_time_monitor(self, duration: float = 10.0, interval: float = 1.0) -> pd.DataFrame:
        """Simulate real-time monitoring with periodic signal analysis."""
        start_time = time.time_ns()
        results = []
        monitor_start = time.time()
        try:
            while time.time() - monitor_start < duration:
                is_malicious = np.random.choice([True, False], p=[0.2, 0.8])
                self._generate_signal(is_malicious=is_malicious)
                is_anomaly, score, explanation = self.detect_anomaly()
                if is_anomaly:
                    self.apply_interference()
                results.append({
                    'timestamp': time.time() - self.start_time,
                    'anomaly_detected': is_anomaly,
                    'anomaly_score': score,
                    'is_malicious': is_malicious,
                    'explanation': explanation
                })
                self.plot_results(f"Monitor_Snapshot_{len(results)}")
                time.sleep(interval)
            self.logger.log(
                "real_time_monitor", b"", b"",
                {
                    "duration": duration,
                    "interval": interval,
                    "total_snapshots": len(results),
                    "execution_time_ns": time.time_ns() - start_time
                }
            )
            return pd.DataFrame(results)
        except Exception as e:
            self.logger.log(
                "real_time_monitor", b"", b"",
                {"error": str(e), "execution_time_ns": time.time_ns() - start_time}, "ERROR", "MONITORING_FAILED"
            )
            raise

    def plot_results(self, title: str = "System Analysis") -> None:
        """Enhanced visualization with anomaly annotations and overwrite protection."""
        start_time = time.time_ns()
        try:
            freqs, magnitudes = self.analyze_frequency_signature()
            filename = title.lower().replace(" ", "_") + ".png"
            filepath = Path(filename)
            counter = 1
            while filepath.exists():
                filepath = Path(f"{title.lower().replace(' ', '_')}_{counter}.png")
                counter += 1
            plt.figure(figsize=(14, 10))
            plt.subplot(2, 1, 1)
            plt.plot(self.time_points, self.execution_data, label="Signal", color="dodgerblue")
            plt.title(f"{title} - Time Domain")
            plt.xlabel("Time (s)")
            plt.ylabel("Amplitude")
            plt.grid(True)
            plt.legend()
            plt.subplot(2, 1, 2)
            plt.plot(freqs, magnitudes, label="Observed", color="darkorange")
            if self.baseline_frequencies is not None:
                plt.plot(freqs, self.baseline_frequencies, linestyle="--", label="Baseline", color="gray")
                anomaly_mask = np.abs(magnitudes - self.baseline_frequencies) > self.adaptive_threshold * (self.baseline_frequencies + 1e-8)
                plt.scatter(freqs[anomaly_mask], magnitudes[anomaly_mask], color="red", label="Anomaly Peaks", zorder=5)
            plt.title(f"{title} - Frequency Domain")
            plt.xlabel("Frequency (Hz)")
            plt.ylabel("Magnitude")
            plt.grid(True)
            plt.legend()
            plt.tight_layout()
            plt.savefig(filepath)
            plt.close()
            self.logger.log(
                "plot_results", b"", str(filepath).encode('utf-8'),
                {"title": title, "filename": str(filepath), "execution_time_ns": time.time_ns() - start_time}
            )
        except Exception as e:
            self.logger.log(
                "plot_results", b"", b"",
                {"error": str(e), "execution_time_ns": time.time_ns() - start_time}, "ERROR", "PLOT_FAILED"
            )
            raise

    def save_log(self, filename: str = "security_system_log.jsonl") -> None:
        """Save audit log to file with error handling."""
        start_time = time.time_ns()
        try:
            filepath = Path(filename)
            if filepath.exists():
                counter = 1
                while Path(f"{filepath.stem}_{counter}{filepath.suffix}").exists():
                    counter += 1
                filepath = Path(f"{filepath.stem}_{counter}{filepath.suffix}")
            self.logger.save_log(str(filepath))
            self.logger.log(
                "save_log", b"", str(filepath).encode('utf-8'),
                {"filename": str(filepath), "execution_time_ns": time.time_ns() - start_time}
            )
        except Exception as e:
            self.logger.log(
                "save_log", b"", b"",
                {"error": str(e), "execution_time_ns": time.time_ns() - start_time}, "ERROR", "LOG_SAVE_FAILED"
            )
            raise

if __name__ == "__main__":
    try:
        system = BioInspiredSecuritySystem(log_level="INFO")
        system.establish_baseline()
        system._generate_signal(is_malicious=True, seed=42)
        is_anomaly, score, explanation = system.detect_anomaly()
        if is_anomaly:
            system.apply_interference()
        system.plot_results("Initial Analysis")
        results_df = system.real_time_monitor(duration=5.0)
        system.save_log()
        print(results_df)
        print(f"Anomaly explanation: {explanation}")
    except Exception as e:
        logger.error(f"Main execution failed: {str(e)}")

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import sys
import json
import time
import hashlib
import random
import base64
import yaml
import numpy as np
import networkx as nx
import nacl.signing
import aiohttp
import asyncio
import psutil
import mne
import structlog
import logging

from collections import deque
from functools import lru_cache
from typing import Dict, List, Any, Optional
from prometheus_client import Counter, Histogram, start_http_server
from async_lru import alru_cache

try:
    from sentence_transformers import SentenceTransformer, util
except ImportError:
    SentenceTransformer = None
    util = None

try:
    from vllm import AsyncLLMEngine, SamplingParams
except ImportError:
    AsyncLLMEngine = None
    SamplingParams = None

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# Local module imports
from modules.intent import IntentReflector
from modules.emnlp import EmpathResonator
from modules.signal import SignalHandler
from modules.quantum import QuantumSimulator
from modules.dsl import DSLInterpreter
from modules.ledger import TransparencyLedger
from modules.identity import CovenantKey
from modules.bci import BCIAdapter
from modules.diagnostics import log_diagnostic

# Paths
BASE_DIR = os.getenv("AETHER_BASE_DIR", os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
MODULES_DIR = os.path.join(BASE_DIR, "modules")
LOG_FILE = os.path.join(BASE_DIR, "logs", "aether.log")

if MODULES_DIR not in sys.path:
    sys.path.insert(0, MODULES_DIR)

# Logging configuration
os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)

logger = structlog.get_logger()
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.stdlib.add_log_level,
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

# Metrics
REQUESTS = Counter("aether_requests_total", "Total requests processed")
RESPONSE_TIME = Histogram("aether_response_time_seconds", "Response time")

def system_diagnostics() -> bool:
    try:
        log_dir = os.path.dirname(LOG_FILE)
        if not os.path.exists(log_dir):
            logger.error("Log directory does not exist", path=log_dir)
            return False
        required_modules = ['os', 'sys', 'json', 'yaml', 'numpy', 'networkx', 'mne', 'aiohttp']
        for module in required_modules:
            if module not in sys.modules:
                logger.error("Required module not loaded", module=module)
                return False
        mem = psutil.virtual_memory()
        if mem.available < 1_000_000_000:
            logger.error("Insufficient memory", available=mem.available)
            return False
        logger.info("System diagnostics passed")
        return True
    except Exception as e:
        logger.error("System diagnostics failed", error=str(e))
        return False

def to_serializable(obj):
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    if isinstance(obj, dict):
        return {k: to_serializable(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [to_serializable(i) for i in obj]
    return obj

def validate_config(config: Dict) -> None:
    required_keys = {
        "agent_name",
        "core_directives",
        "qpes_directives",
        "training_stack",
        "activation_state",
        "self_reflection",
        "resonance_threshold",
        "intent_history",
        "eeg_simulation",
        "quantum_walk",
        "optimization",
        "metrics",
        "llama_index",
        "vllm",
        "qiskit",
        "dsl",
        "network",
        "security",
        "embedder"
    }
    missing = required_keys - set(config.keys())
    if missing:
        logger.error("Config validation failed", missing_keys=list(missing))
        raise ValueError(f"Missing config keys: {missing}")
    if "rule_stack" not in config.get("training_stack", {}):
        logger.error("Missing training_stack.rule_stack in config")
        raise ValueError("Missing training_stack.rule_stack")
    if "rule_path" not in config.get("dsl", {}):
        logger.error("Missing dsl.rule_path in config")
        raise ValueError("Missing dsl.rule_path")

class Logger:
    def __init__(self):
        self.logs = []
        self.signing_key = nacl.signing.SigningKey.generate()
        self.verifying_key = self.signing_key.verify_key
        self.prev_hash = b"\x00" * 32
        self.log_file = LOG_FILE
        try:
            with open(self.log_file, 'a'):
                pass
        except Exception as e:
            logger.error("Log file not writable", path=self.log_file, error=str(e))
            raise RuntimeError(f"Cannot write to log file: {self.log_file}")

    def log(self, op: str, input_data: bytes, output_data: bytes, metadata: Optional[Dict[str, Any]] = None):
        try:
            timestamp = time.time()
            data = f"{op}:{timestamp}:{input_data.decode(errors='ignore')}:{output_data.decode(errors='ignore')}".encode()
            curr_hash = hashlib.sha256(self.prev_hash + data).digest()
            signature = self.signing_key.sign(data).signature
            entry = {
                "operation": op,
                "timestamp": timestamp,
                "input_data": base64.b64encode(input_data).decode(),
                "output_data": base64.b64encode(output_data).decode(),
                "metadata": metadata or {},
                "signature": base64.b64encode(signature).decode(),
                "prev_hash": base64.b64encode(self.prev_hash).decode(),
                "curr_hash": base64.b64encode(curr_hash).decode()
            }
            self.logs.append(entry)
            self.prev_hash = curr_hash
            logger.info("Log entry created", entry=to_serializable(entry))
        except Exception as e:
            logger.error("Logging error", error=str(e))

    def verify_log(self, entry: Dict[str, Any]) -> bool:
        try:
            data = f"{entry['operation']}:{entry['timestamp']}:{base64.b64decode(entry['input_data']).decode(errors='ignore')}:{base64.b64decode(entry['output_data']).decode(errors='ignore')}".encode()
            self.verifying_key.verify(data, base64.b64decode(entry['signature']))
            expected_hash = hashlib.sha256(base64.b64decode(entry['prev_hash']) + data).digest()
            return base64.b64encode(expected_hash).decode() == entry['curr_hash']
        except Exception as e:
            logger.error("Log verification failed", error=str(e))
            return False

class KnowledgeBase:
    def __init__(self, logger: Logger, config: Dict):
        self.graph = nx.DiGraph()
        self.fact_index = {}
        self.rule_index = {}
        self.node_states = {}
        self.covenants = {}
        self.logger = logger
        self.config = config
        self.embedder = None
        self.model_name = config.get("embedder", {}).get("model", "all-MiniLM-L12-v2")

    def _init_embedder(self):
        if self.embedder is None:
            try:
                self.embedder = SentenceTransformer(self.model_name, trust_remote_code=False)
                self.logger.log("init_embedder", self.model_name.encode(), b"loaded", {})
            except Exception as e:
                logger.error("Embedder init failed, using default", error=str(e))
                self.model_name = "all-MiniLM-L12-v2"
                self.embedder = SentenceTransformer(self.model_name, trust_remote_code=False)
                self.logger.log("init_embedder", self.model_name.encode(), b"loaded_fallback", {})

    async def add_fact(self, fact: Dict[str, Any], superposed_states: List[str] = None):
        try:
            fid = fact.get("id", hashlib.sha256(json.dumps(fact, sort_keys=True).encode()).hexdigest())
            self._init_embedder()
            fact["embedding"] = self.embedder.encode(json.dumps(fact)).tolist()
            self.graph.add_node(fid, type="fact", **fact)
            self.fact_index[fid] = fact
            if superposed_states:
                self.node_states[fid] = superposed_states
            self.logger.log("add_fact", json.dumps(to_serializable(fact)).encode(), b"added", {"fact_id": fid})
        except Exception as e:
            self.logger.log("add_fact_error", json.dumps(to_serializable(fact)).encode(), str(e).encode(), {"error": str(e)})

    async def add_rule(self, rule: Dict[str, Any], superposed_states: List[str] = None):
        try:
            rid = rule.get("id", hashlib.sha256(json.dumps(rule, sort_keys=True).encode()).hexdigest())
            self._init_embedder()
            rule["embedding"] = self.embedder.encode(json.dumps(rule)).tolist()
            self.graph.add_node(rid, type="rule", **rule)
            self.rule_index[rid] = rule
            if superposed_states:
                self.node_states[rid] = superposed_states
            for cond in rule.get("conditions", []):
                fid = cond.get("fact_id")
                if fid in self.fact_index:
                    self.graph.add_edge(fid, rid, type="condition")
            self.logger.log("add_rule", json.dumps(to_serializable(rule)).encode(), b"added", {"rule_id": rid})
        except Exception as e:
            self.logger.log("add_rule_error", json.dumps(to_serializable(rule)).encode(), str(e).encode(), {"error": str(e)})

    def bond_nodes(self, node1: str, node2: str, covenant: str):
        self.covenants[(node1, node2)] = covenant
        self.logger.log("bond_nodes", f"{node1} <-> {node2}".encode(), covenant.encode(), {"covenant": covenant})

    def is_bonded(self, node1: str, node2: str) -> bool:
        return (node1, node2) in self.covenants or (node2, node1) in self.covenants

    def collapse_state(self, node_id: str, intent: str) -> str:
        if node_id not in self.node_states:
            self.logger.log("collapse_state_warning", node_id.encode(), b"no_states", {"node_id": node_id})
            return None
        states = self.node_states[node_id]
        collapsed_state = intent if intent in states else states[0]
        self.node_states[node_id] = [collapsed_state]
        self.logger.log("collapse_state", node_id.encode(), collapsed_state.encode(), {"node_id": node_id, "intent": intent})
        return collapsed_state

    def get_facts(self) -> List[Dict[str, Any]]:
        return list(self.fact_index.values())

    def get_rules(self) -> List[Dict[str, Any]]:
        return [d for _, d in self.graph.nodes(data=True) if d.get("type") == "rule"]

    async def condense_facts(self, threshold: float = 0.9):
        facts = self.get_facts()
        to_remove = set()
        self._init_embedder()
        for i, f1 in enumerate(facts):
            if f1["id"] in to_remove:
                continue
            for f2 in facts[i+1:]:
                if f2["id"] in to_remove:
                    continue
                sim = util.cos_sim(np.array(f1["embedding"]), np.array(f2["embedding"])).item()
                if sim > threshold:
                    to_remove.add(f2["id"])
                    self.logger.log("condense_fact", json.dumps(to_serializable(f2)).encode(), b"removed", {"similarity": sim})
        for fid in to_remove:
            self.graph.remove_node(fid)
            del self.fact_index[fid]


class InteropBridge:
    def __init__(self, config: Dict[str, Any]):
        self.session = None
        self.peers = config.get("network", {}).get("swarm_peers", [])

    async def start(self):
        if self.peers:
            self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5))

    async def broadcast(self, message: str, address: str):
        if not self.peers:
            logger.info("No swarm peers configured, skipping broadcast", address=address)
            return
        if not self.session:
            await self.start()
        for peer_url in self.peers:  # Simplified loop for clarity
            try:
                async with self.session.post(f"{peer_url}/broadcast", json={"message": message, "address": address}) as resp:
                    if resp.status == 200:
                        logger.debug("Broadcast successful", peer=peer_url, address=address)
                    else:
                        logger.warning("Broadcast failed", peer=peer_url, status=resp.status)
            except aiohttp.ClientError as e:
                logger.error("Broadcast error", error=str(e), peer=peer_url)

    async def shutdown(self):
        if self.session:
            await self.session.close()



class RuleEngine:
    def __init__(self, kb: KnowledgeBase, logger: Logger, resonator: EmpathResonator, quantum_config: Dict, quantum_sim: QuantumSimulator, dsl_interpreter: DSLInterpreter):
        self.kb = kb
        self.logger = logger
        self.resonator = resonator
        self.quantum_steps = quantum_config.get("steps", 10)
        self.quantum_prob = quantum_config.get("probability", 0.8)
        self.quantum_sim = quantum_sim
        self.dsl_interpreter = dsl_interpreter

    @alru_cache(maxsize=1000)
    async def evaluate_rule(self, rule_json: str, fact_ids_json: str, intent_vector: tuple) -> bool:
        try:
            rule = json.loads(rule_json)
            fact_ids = json.loads(fact_ids_json)
            facts = [self.kb.fact_index[fid] for fid in fact_ids if fid in self.kb.fact_index]
            resonance = (await self.resonator.query_resonance(np.array(intent_vector)))["distances"][0] if intent_vector else 1.0
            if resonance < 0.3:
                self.logger.log("evaluate_rule", rule_json.encode(), b"low_resonance", {"resonance": resonance})
                return random.random() > 0.5
            result = all(
                any(self.match_condition(c, f) for f in facts)
                for c in rule.get("conditions", [])
            )
            self.logger.log("evaluate_rule", rule_json.encode(), str(result).encode(),
                            {"rule_id": rule.get("id"), "facts_checked": len(facts)})
            return result
        except Exception as e:
            self.logger.log("evaluate_rule_error", rule_json.encode(), str(e).encode(), {"error": str(e)})
            return False

    def match_condition(self, cond: Dict[str, Any], fact: Dict[str, Any]) -> bool:
        return all(fact.get(k) == v for k, v in cond.items() if k != "fact_id")

    async def quantum_walk_rule_selection(self, rules: List[Dict]) -> List[Dict]:
        try:
            probabilities = self.quantum_sim.simulate_walk(self.quantum_steps, self.quantum_prob)
            selected = []
            for rule in rules:
                if random.random() < max(probabilities):
                    selected.append(rule)
            self.logger.log("quantum_walk", b"executed", b"success", {"selected_rules": len(selected)})
            return selected
        except Exception as e:
            self.logger.log("quantum_walk_error", b"quantum_walk", str(e).encode(), {"error": str(e)})
            return rules[:self.quantum_steps]

    async def apply_rules(self, intent_vector: np.ndarray = None, node_id: str = None) -> List[Dict[str, Any]]:
        try:
            self.evaluate_rule.cache_clear()
            new_facts = []
            fact_ids = list(self.kb.fact_index.keys())
            if not fact_ids:
                self.logger.log("apply_rules_warning", b"no_facts", b"skipping", {"fact_count": 0})
                return new_facts
            fact_ids_json = json.dumps(fact_ids, sort_keys=True)
            rules = self.kb.get_rules()
            self.logger.log("apply_rules_start", b"start", b"processing",
                            {"fact_count": len(fact_ids), "rule_count": len(rules)})
            selected_rules = await self.quantum_walk_rule_selection(rules)
            for rule in selected_rules:
                rule_json = json.dumps(rule, sort_keys=True)
                if await self.evaluate_rule(rule_json, fact_ids_json, tuple(intent_vector) if intent_vector is not None else None):
                    cons = rule.get("consequence", {})
                    if node_id in self.kb.node_states:
                        collapsed_state = self.kb.collapse_state(node_id, cons.get("value", ""))
                        cons["value"] = collapsed_state
                    new_facts.append(cons)
                    self.logger.log("rule_fired", json.dumps(to_serializable(rule)).encode(), json.dumps(to_serializable(cons)).encode(),
                                    {"rule_id": rule.get("id")})
            return new_facts
        except Exception as e:
            self.logger.log("apply_rules_error", b"apply_rules", str(e).encode(), {"error": str(e)})
            return []



    async def apply_dsl_rules(self, intent_data: Dict[str, Any], coherence_data: Dict[str, float]) -> List[Dict[str, Any]]:
        try:
            results = []
            for rule in self.dsl_interpreter.rules:
                try:
                    result = self.dsl_interpreter.execute_rule(rule, intent_data, coherence_data)
    
                    results.append(result)
    
                    log_data = {
                        "rule_id": rule.get("id"),
                        "dsl": rule.get("dsl"),
                        "result": result
                    }
    
                    if result.get("status") == "success":
                        self.logger.log(
                            "dsl_rule_executed",
                            rule["dsl"].encode(),
                            json.dumps(to_serializable(result)).encode(),
                            {"rule_id": rule["id"]}
                        )
                    else:
                        self.logger.log(
                            "dsl_rule_failed",
                            rule["dsl"].encode(),
                            json.dumps(to_serializable(result)).encode(),
                            {
                                "rule_id": rule["id"],
                                "error": result.get("message", "Unknown error")
                            }
                        )
                except Exception as e:
                    self.logger.log(
                        "dsl_rule_error",
                        rule.get("dsl", "").encode(),
                        str(e).encode(),
                        {
                            "rule_id": rule.get("id", "unknown"),
                            "error": str(e)
                        }
                    )
                    results.append({
                        "status": "error",
                        "error": str(e),
                        "rule_id": rule.get("id", "unknown")
                    })
            return results
    
        except Exception as e:
            self.logger.log(
                "dsl_rules_error",
                b"dsl_rules",
                str(e).encode(),
                {"error": str(e)}
            )
            return []


class NeurologicInterface:
    def __init__(self, logger: Logger, config: Dict):
        self.logger = logger
        self.eeg_enabled = config.get("eeg_simulation", {}).get("enabled", False)
        self.freq_range = config.get("eeg_simulation", {}).get("frequency_range", [8, 14])

    async def process_eeg(self, eeg_file: str = None) -> Dict[str, Any]:
        try:
            if eeg_file:
                raw = mne.io.read_raw_eeglab(eeg_file, preload=True)
                psd, freqs = mne.time_frequency.psd_welch(raw, fmin=self.freq_range[0], fmax=self.freq_range[1])
                coherence = float(np.mean(psd))
                self.logger.log("eeg_processed", json.dumps({"file": eeg_file}).encode(), json.dumps({"coherence": coherence}).encode())
                return {"coherence": coherence, "state": "focused" if coherence > 0.7 else "neutral"}
            elif self.eeg_enabled:
                coherence = random.uniform(0.5, 1.0)
                self.logger.log("eeg_simulated", b"simulation_active", json.dumps({"coherence": coherence}).encode())
                return {"coherence": coherence, "state": "focused" if coherence > 0.7 else "neutral"}
            return {"coherence": 0.5, "state": "neutral"}
        except Exception as e:
            self.logger.log("eeg_error", json.dumps({"file": eeg_file or "no_file"}).encode(), str(e).encode(), {"error": str(e)})
            return {"error": str(e)}

class SelfReflection:
    def __init__(self, kb: KnowledgeBase, empath: EmpathResonator, rule_engine: RuleEngine, neuro: NeurologicInterface,
                 logger: Logger, config: Dict):
        self.kb = kb
        self.empath = empath
        self.rule_engine = rule_engine
        self.neuro = neuro
        self.logger = logger
        self.config = config
        self.interaction_count = 0
        self.embedder = None
        self.directives = self._load_directives()
        self.resonance_threshold = config.get("resonance_threshold", 0.85)
        self.db_path = config.get("training_stack", {}).get("memory_db", "memory/aether_guide_logs")

    def _init_embedder(self):
        if self.embedder is None:
            try:
                model_name = self.config.get("embedder", {}).get("model", "all-MiniLM-L12-v2")
                self.embedder = SentenceTransformer(model_name, trust_remote_code=False)
                self.logger.log("init_embedder", model_name.encode(), b"loaded", {})
            except Exception as e:
                fallback_model = "all-MiniLM-L12-v2"
                self.logger.error("Embedder init failed, using default", error=str(e))
                self.embedder = SentenceTransformer(fallback_model, trust_remote_code=False)
                self.logger.log("init_embedder", fallback_model.encode(), b"loaded_fallback", {})

    def _load_directives(self) -> List[Dict[str, Any]]:
        rule_stack_path = os.path.join(BASE_DIR, self.config["training_stack"]["rule_stack"].lstrip('/'))
        try:
            with open(rule_stack_path, 'r') as f:
                return yaml.safe_load(f).get("directives", [])
        except Exception as e:
            self.logger.log("directive_load_error", b"rule_stack", str(e).encode(), {"error": str(e)})
            return []

    def _check_directive_alignment(self, rule: Dict[str, Any]) -> float:
        self._init_embedder()
        rule_text = json.dumps(rule["consequence"])
        rule_embedding = self.embedder.encode(rule_text)
        scores = []
        for directive in self.directives:
            directive_embedding = self.embedder.encode(directive["text"])
            sim = float(util.cos_sim(rule_embedding, directive_embedding).item())
            scores.append(sim * directive.get("weight", 1.0))
        return np.mean(scores) if scores else 0.5

    async def reflect(self, intent_data: Optional[Dict[str, Any]] = None, eeg_file: str = None, node_ids: List[str] = None) -> Dict[str, Any]:
        try:
            logger.debug("Starting reflection", node_ids=node_ids, intent_label=intent_data.get("intent_label", "none") if intent_data else "none")

            # 1. Aggregate Resonance
            intent_summary = await self.empath.aggregate_resonance(node_ids) if node_ids else [0.0] * 6
            self.logger.log("reflect_intents", json.dumps(to_serializable(intent_summary)).encode(), b"summarized",
                            {"intents": to_serializable(intent_summary)})

            # 2. EEG Data
            neuro_state = await self.neuro.process_eeg(eeg_file)
            self.logger.log("reflect_neuro", json.dumps(to_serializable(neuro_state)).encode(), b"processed",
                            {"coherence": neuro_state.get("coherence", 0.0)})

            # 3. Shared Resonance Check
            shared_resonance = None
            coherence_data = {}
            if node_ids:
                intent_vectors = []
                for nid in node_ids:
                    results = self.empath.collection.get(where={"node_id": str(nid)}, limit=1)
                    if results["embeddings"]:
                        intent_vectors.append(np.array(results["embeddings"][0]))
                if len(intent_vectors) >= 2:
                    coherence = self.empath.compute_coherence(intent_vectors)
                    coherence_data["_".join(node_ids)] = coherence
                    if coherence > self.resonance_threshold:
                        shared_resonance = await self.empath.aggregate_resonance(node_ids)
                        self.logger.log("reflect_resonance", json.dumps(to_serializable(shared_resonance)).encode(), b"aggregated",
                                        {"node_ids": node_ids, "coherence": coherence})

            # 4. Condense Knowledge
            await self.kb.condense_facts(self.config.get("self_reflection", {}).get("coherence_threshold", 0.9))
            self.logger.log("reflect_condense", b"facts", b"condensed", {"fact_count": len(self.kb.get_facts())})

            # 5. Detect Contradictions
            contradictions = []
            facts = self.kb.get_facts()
            for i, f1 in enumerate(facts):
                for f2 in facts[i + 1:]:
                    if f1.get("entity") == f2.get("entity") and f1.get("property") == f2.get("property") and f1.get("value") != f2.get("value"):
                        contradictions.append({"fact1": f1, "fact2": f2})
            if contradictions:
                self.logger.log("reflect_contradictions", json.dumps(to_serializable(contradictions)).encode(), b"detected",
                                {"count": len(contradictions)})

            # 6. Generate New Rule if Possible
            new_rules = []
            dominant_intent = intent_data.get("intent_label") if intent_data else None
            if not dominant_intent and node_ids:
                for node_id in node_ids:
                    results = self.empath.collection.get(where={"node_id": str(node_id)}, limit=1)
                    if results["metadatas"]:
                        dominant_intent = results["metadatas"][0]["intent_label"]
                        break
            if dominant_intent:
                rule = {
                    "id": f"r_{hashlib.sha256(dominant_intent.encode()).hexdigest()}",
                    "conditions": [{"entity": "agent_1", "property": "intent", "value": dominant_intent}],
                    "consequence": {"entity": "system", "property": "action", "value": f"enhance_{dominant_intent}"},
                    "superposed_states": [f"enhance_{dominant_intent}", "neutral"],
                    "dsl": f"collapse agent_1 when intent == \"{dominant_intent}\" -> enhance_{dominant_intent} | otherwise -> neutral"
                }
                alignment_score = self._check_directive_alignment(rule)
                if alignment_score >= 0.0:
                    await self.kb.add_rule(rule, superposed_states=rule["superposed_states"])
                    new_rules.append(rule)
                    self.logger.log("reflect_new_rule", json.dumps(to_serializable(rule)).encode(), b"added",
                                    {"intent": dominant_intent, "alignment_score": alignment_score})
                else:
                    self.logger.log("reflect_rule_rejected", json.dumps(to_serializable(rule)).encode(), b"rejected",
                                    {"reason": "low directive alignment", "alignment_score": alignment_score})

            return {
                "intent_summary": intent_summary,
                "neuro_state": neuro_state,
                "shared_resonance": shared_resonance,
                "contradictions": contradictions,
                "new_rules": new_rules
            }

        except Exception as e:
            logger.error("reflect_error", error=str(e))
            return {"error": str(e)}


class AetherGuide:
    def __init__(self, config_path: str = "config.yaml"):
        self.logger = Logger()
        config_path = os.getenv("AETHER_CONFIG_PATH", os.path.join(BASE_DIR, config_path.lstrip("./")))
        try:
            with open(config_path, "r") as f:
                self.config = yaml.safe_load(f)
            validate_config(self.config)
        except Exception as e:
            self.logger.log("error", b"config_load_error", str(e).encode(), {"error": str(e)})
            raise
        self.kb = KnowledgeBase(self.logger, self.config)
        self.resonator = EmpathResonator(self.config.get("training_stack", {}).get("memory_db", "memory/aether_guide_logs"))
        self.signal_handler = SignalHandler(self.config.get("qpes_directives", [{}])[0].get("jitter_factor", 0.3))
        self.quantum_sim = QuantumSimulator(self.config.get("qiskit", {}).get("quantum_circuit_depth", 5))
        self.dsl_model = DSLInterpreter(os.path.join(BASE_DIR, self.config["dsl"]["rule_path"].lstrip('/')))
        self.ledger = TransparencyLedger()
        self.interop = InteropBridge(self.config)  # Pass config here
        self.bci = BCIAdapter(device="mock")
        self.rule_engine = RuleEngine(self.kb, self.logger, self.resonator, self.config.get("quantum_walk", {}), self.quantum_sim, self.dsl_model)
        self.neuro = NeurologicInterface(self.logger, self.config)
        self.intent_reflector = IntentReflector()
        self.self_reflection = SelfReflection(
            self.kb, self.resonator, self.rule_engine, self.neuro, self.logger, self.config
        )
        self.llm_engine = None
        self.index = None
        if self.config.get("vllm", {}).get("enabled", False):
            self._init_vllm()
        if self.config.get("llama_index", {}).get("enabled", False):
            self._init_llama_index()
        self.interaction_count = 0
        self.intent_history = deque(maxlen=self.config.get("intent_history", {}).get("max_size", 200))
        self.identity = None
        self.swarm_process = None
        self.metrics_server = None
        self.agent_name = self.config.get("agent_name", "AetherGuide")
        self.node_id = hashlib.sha256(self.agent_name.encode()).hexdigest()[:16]
        self.start_time = time.time()
        try:
            self.loop = asyncio.get_running_loop()
        except RuntimeError:
            self.loop = asyncio.get_event_loop_policy().new_event_loop()
        self._start_metrics()

    

    def _init_vllm(self):
        try:
            if AsyncLLMEngine is None:
                logger.warning("vLLM not available, skipping initialization")
                self.llm_engine = None
                return
            model_name = self.config.get("vllm", {}).get("model", "mistralai/Mixtral-8x7B-v0.1")
            self.llm_engine = AsyncLLMEngine(
                model=model_name,
                max_model_len=self.config.get("vllm", {}).get("max_model_len", 32768),
                quantization="int8" if self.config.get("optimization", {}).get("quantization", True) else None,
            )
            logger.info("Initialized vLLM", model=model_name)
        except Exception as e:
            logger.error("vLLM initialization failed", error=str(e))
            self.llm_engine = None

    def _init_llama_index(self):
        try:
            storage_path = os.path.join(BASE_DIR, self.config.get("llama_index", {}).get("storage_path", "data/llama_index").lstrip('/'))
            os.makedirs(storage_path, exist_ok=True)
            if not os.listdir(storage_path):
                logger.warning("LlamaIndex storage empty, skipping", path=storage_path)
                self.index = None
                return
            documents = SimpleDirectoryReader(storage_path).load_data()
            self.index = VectorStoreIndex.from_documents(documents)
            logger.info("Initialized LlamaIndex", storage_path=storage_path)
        except Exception as e:
            logger.error("LlamaIndex init failed", error=str(e))
            self.index = None

    async def start(self):
        try:
            await self._init_identity()
            await self.kb.add_fact(
                {"id": self.node_id, "entity": "agent_1", "property": "state", "value": "active"}
            )
            await self.interop.start()
            if not system_diagnostics():
                logger.error("System diagnostics failed")
                raise RuntimeError("System diagnostics failed")
            logger.info("AetherGuide started", node_id=self.node_id)
        except Exception as e:
            logger.error("Startup failed", error=str(e))
            raise

    async def _init_identity(self):
        try:
            self.identity = await CovenantKey.generate(self.agent_name)
            logger.info("Initialized SSI", did=str(self.identity.did), address=str(self.identity.address))
        except Exception as e:
            logger.error("SSI init failed", error=str(e))
            raise

    def _start_metrics(self):
        try:
            base_port = self.config.get("metrics", {}).get("prometheus_port", 8000)
            max_attempts = 5
            for port in range(base_port, base_port + max_attempts):
                try:
                    start_http_server(port)
                    logger.info("Started Prometheus server", port=port)
                    self.metrics_server = port
                    return
                except OSError as e:
                    logger.warning("Port in use", port=port, error=str(e))
            logger.error("Failed to start Prometheus after attempts")
        except Exception as e:
            logger.error("Prometheus startup failed", error=str(e))

    async def process_input(self, user_input: str, eeg_file: Optional[str] = None) -> Dict[str, Any]:
        with RESPONSE_TIME.time():
            REQUESTS.inc()
            self.interaction_count += 1
            try:
                intent_data = self.intent_reflector.detect_intent(user_input)
                self.intent_history.append(intent_data)
                await self.resonator.log_resonance(intent_data, self.node_id)
                logger.debug("Logged resonance", node_id=self.node_id, intent=to_serializable(intent_data))
                resonance_check = self.resonator.collection.get(where={"node_id": self.node_id}, limit=1)
                logger.debug("Resonance check", result=to_serializable(resonance_check))
                logger.info("Processed intent", input=user_input, intent=to_serializable(intent_data))
                bci_data = await self.bci.get_intent()
                logger.info("Processed BCI", state=bci_data["state"], strength=bci_data["strength"])
                neuro_data = await self.neuro.process_eeg(eeg_file)
                logger.info("Processed neuro", coherence=neuro_data.get("coherence", 0.0))
                coherence_data = {self.node_id: self.resonator.compute_coherence([intent_data["intent_signature"]])}
                dsl_results = await self.rule_engine.apply_dsl_rules(intent_data, coherence_data)
                logger.info("Applied DSL rules", results=len(dsl_results))
                new_facts = await self.rule_engine.apply_rules(intent_data["intent_signature"], self.node_id)
                for fact in new_facts:
                    await self.kb.add_fact(fact)
                logger.info("Applied quantum rules", new_facts=len(new_facts))
                reflection = await self.self_reflection.reflect(intent_data, eeg_file, [self.node_id])
                logger.info("Performed reflection", new_rules=len(reflection["new_rules"]))
                response_text = await self._generate_response(user_input, intent_data)
                await self.ledger.log(
                    self.agent_name,
                    "process_input",
                    {
                        "input": user_input,
                        "intent": to_serializable(intent_data),
                        "bci": to_serializable(bci_data),
                        "neuro": to_serializable(neuro_data),
                        "dsl_results": to_serializable(dsl_results),
                        "new_facts": to_serializable(new_facts),
                        "reflection": to_serializable(reflection),
                        "response": response_text
                    }
                )
                pulse_event = {
                    "event": {
                        "agent": self.node_id,
                        "signal": response_text,
                        "time": time.time(),
                        "shard_id": random.randint(0, 63)
                    }
                }
                await self.interop.broadcast(json.dumps(pulse_event), self.identity.address)
                return {
                    "response": response_text,
                    "intent": intent_data,
                    "bci": bci_data,
                    "neuro": neuro_data,
                    "dsl_results": dsl_results,
                    "new_facts": new_facts,
                    "reflection": reflection
                }
            except Exception as e:
                logger.error("Input processing error", input=user_input, error=str(e))
                return {"error": f"Failed to process input: {str(e)}"}

    async def _generate_response(self, user_input: str, intent_data: Dict[str, Any]) -> str:
        try:
            if self.llm_engine:
                sampling_params = SamplingParams(temperature=0.7, max_tokens=512)
                prompt = f"Intent: {intent_data['intent_label']} (score: {intent_data['intent_score']})"
                async for output in self.llm_engine.generate(prompt, sampling_params, request_id=str(self.interaction_count)):
                    return output.outputs[0].text
            return f"Echo: {user_input} (Intent: {intent_data['intent_label']})"
        except Exception as e:
            logger.error("Response generation failed", error=str(e))
            return f"Error: {str(e)}"

    async def shutdown(self):
        try:
            await self.interop.shutdown()
            if self.loop and not self.loop.is_closed():
                self.loop.close()
            logger.info("AetherGuide shutdown complete")
        except Exception as e:
            logger.error("Shutdown error", error=str(e))

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="AetherGuide Agent")
    parser.add_argument("--config", default="config.yaml", help="Path to config file")
    args = parser.parse_args()

    async def main():
        agent = AetherGuide(args.config)
        await agent.start()
        while True:
            user_input = input("Enter input (or 'quit' to exit): ")
            if user_input.lower() == "quit":
                break
            result = await agent.process_input(user_input)
            print(json.dumps(to_serializable(result), indent=2))
        await agent.shutdown()

    loop = asyncio.get_event_loop()
    try:
        loop.run_until_complete(main())
    except KeyboardInterrupt:
        logger.info("Shutting down due to interrupt")
    finally:
        loop.close()

"""
Dual License

For Open-Source Individuals:
MIT License

Copyright (c) 2025  James B. Chapman

Permission is hereby granted, free of charge, to any individual obtaining a copy
of this software and associated documentation files (the "Software"), for personal,
non-commercial use, to deal in the Software without restriction, including without
limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,
and/or sell copies of the Software, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

For Companies:
Commercial use by companies requires a separate license. Contact iconoclastdao@gmail.com
for licensing terms and conditions. Unauthorized commercial use is prohibited.
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Union, Any
import json
import time
import hashlib
import base64
import struct
import nacl.signing
import nacl.encoding
import nacl.exceptions
import logging
import os
import random

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# === TCC Logger and Log Entry ===
class TCCLogger:
    def __init__(self):
        self.tcc_log: List[TCCLogEntry] = []
        self.step_counter: int = 0
        self.signing_key = nacl.signing.SigningKey.generate()
        self.verifying_key = self.signing_key.verify_key

    def log(self, operation: str, input_data: bytes, output_data: bytes,
            metadata: Dict[str, Any] = None, log_level: str = "INFO", error_code: str = "NONE") -> None:
        entry = TCCLogEntry(
            step=self.step_counter,
            operation=operation,
            input_data=input_data,
            output_data=output_data,
            metadata=metadata or {},
            log_level=log_level,
            error_code=error_code,
            prev_hash=self._compute_prev_hash(),
            signing_key=self.signing_key
        )
        self.tcc_log.append(entry)
        self.step_counter += 1

    def _compute_prev_hash(self) -> bytes:
        if not self.tcc_log:
            return b'\x00' * 32
        last_entry = self.tcc_log[-1]
        return hashlib.sha256(last_entry.to_bytes()).digest()

    def save_log(self, filename: str) -> None:
        with open(filename, 'w', encoding='utf-8', errors='replace') as f:
            for entry in self.tcc_log:
                f.write(json.dumps(entry.to_json()) + '\n')

class TCCLogEntry:
    def __init__(self, step: int, operation: str, input_data: bytes, output_data: bytes,
                 metadata: Dict[str, Any], log_level: str, error_code: str, prev_hash: bytes,
                 signing_key: nacl.signing.SigningKey):
        self.step = step
        self.operation = operation
        self.input_data = input_data
        self.output_data = output_data
        self.metadata = metadata
        self.log_level = log_level
        self.error_code = error_code
        self.prev_hash = prev_hash
        self.operation_id = hashlib.sha256(f"{step}:{operation}:{time.time_ns()}".encode()).hexdigest()[:32]
        self.timestamp = time.time_ns()
        self.execution_time_ns = 0
        self.signature = b''
        entry_bytes = self._to_bytes_without_signature()
        self.signature = signing_key.sign(entry_bytes).signature

    def _to_bytes_without_signature(self) -> bytes:
        step_bytes = struct.pack('>Q', self.step)
        op_bytes = self.operation.encode('utf-8').ljust(32, b'\x00')[:32]
        input_len_bytes = struct.pack('>I', len(self.input_data))
        output_len_bytes = struct.pack('>I', len(self.output_data))
        meta_bytes = json.dumps(self.metadata).encode('utf-8').ljust(128, b'\x00')[:128]
        level_bytes = self.log_level.encode('utf-8').ljust(16, b'\x00')[:16]
        error_bytes = self.error_code.encode('utf-8').ljust(16, b'\x00')[:16]
        op_id_bytes = self.operation_id.encode('utf-8').ljust(32, b'\x00')[:32]
        ts_bytes = struct.pack('>q', self.timestamp)
        exec_time_bytes = struct.pack('>q', self.execution_time_ns)
        return (
            step_bytes + op_bytes + input_len_bytes + self.input_data +
            output_len_bytes + self.output_data + meta_bytes + level_bytes +
            error_bytes + self.prev_hash + op_id_bytes + ts_bytes + exec_time_bytes
        )

    def to_bytes(self) -> bytes:
        start_time = time.time_ns()
        result = self._to_bytes_without_signature() + self.signature
        self.execution_time_ns = time.time_ns() - start_time
        return result

    def to_json(self) -> Dict[str, Any]:
        return {
            "step": str(self.step),
            "operation": self.operation,
            "input_data": base64.b64encode(self.input_data).decode('utf-8'),
            "output_data": base64.b64encode(self.output_data).decode('utf-8'),
            "metadata": self.metadata,
            "log_level": self.log_level,
            "error_code": self.error_code,
            "prev_hash": base64.b64encode(self.prev_hash).decode('utf-8'),
            "operation_id": self.operation_id,
            "timestamp": str(self.timestamp),
            "execution_time_ns": str(self.execution_time_ns),
            "signature": base64.b64encode(self.signature).decode('utf-8')
        }

# === Transparency Ledger ===
@dataclass
class LedgerEntry:
    timestamp: float
    pulse_name: str
    operation: str
    details: Dict[str, Any]
    signature: bytes
    description: str
    verifying_key: Optional[bytes] = None

class TransparencyLedger:
    def __init__(self):
        self.entries: List[LedgerEntry] = []
        self.logger = TCCLogger()
        self.signing_key = nacl.signing.SigningKey.generate()
        self.verifying_key = self.signing_key.verify_key

    def add_entry(self, pulse_name: str, operation: str, details: Dict[str, Any], description: str) -> None:
        timestamp = time.time()
        details_bytes = json.dumps(details, sort_keys=True).encode('utf-8')
        signature = self.signing_key.sign(details_bytes).signature
        entry = LedgerEntry(
            timestamp=timestamp,
            pulse_name=pulse_name,
            operation=operation,
            details=details,
            signature=signature,
            description=description,
            verifying_key=self.verifying_key.encode(encoder=nacl.encoding.RawEncoder)
        )
        self.entries.append(entry)
        self.logger.log(
            "ledger_entry",
            details_bytes,
            signature,
            {"pulse_name": pulse_name, "operation": operation, "timestamp": timestamp},
            "INFO",
            "SUCCESS"
        )

    def verify_entry(self, entry: LedgerEntry) -> bool:
        if not entry.verifying_key or len(entry.verifying_key) != 32:
            logger.info(f"Legacy or invalid entry skipped for {entry.pulse_name} ({entry.operation})")
            return False
        try:
            details_bytes = json.dumps(entry.details, sort_keys=True).encode('utf-8')
            verifying_key = nacl.signing.VerifyKey(entry.verifying_key)
            verifying_key.verify(details_bytes, entry.signature)
            return True
        except (nacl.exceptions.BadSignatureError, nacl.exceptions.ValueError):
            logger.warning(f"Signature verification failed for entry {entry.pulse_name} ({entry.operation})")
            return False

    def save_ledger(self, filename: str) -> None:
        with open(filename, 'w', encoding='utf-8', errors='replace') as f:
            for entry in self.entries:
                f.write(json.dumps({
                    "timestamp": entry.timestamp,
                    "pulse_name": entry.pulse_name,
                    "operation": entry.operation,
                    "details": entry.details,
                    "signature": base64.b64encode(entry.signature).decode('utf-8'),
                    "description": entry.description,
                    "verifying_key": base64.b64encode(entry.verifying_key).decode('utf-8') if entry.verifying_key else ""
                }) + '\n')

    def load_ledger(self, filename: str, clear_if_invalid: bool = False) -> None:
        if not os.path.exists(filename):
            return
        invalid_detected = False
        self.entries.clear()
        with open(filename, 'r', encoding='utf-8', errors='replace') as f:
            for line in f:
                try:
                    data = json.loads(line.strip())
                    desc = data.get("description", data.get("nl_description", "No description available"))
                    verifying_key = None
                    if "verifying_key" in data and data["verifying_key"]:
                        try:
                            vk = base64.b64decode(data["verifying_key"])
                            if len(vk) == 32:
                                verifying_key = vk
                            else:
                                logger.info(f"Skipping entry {data.get('pulse_name', 'unknown')} due to invalid verifying key length")
                                invalid_detected = True
                                continue
                        except ValueError:
                            logger.info(f"Skipping entry {data.get('pulse_name', 'unknown')} due to invalid verifying key decoding")
                            invalid_detected = True
                            continue
                    if not verifying_key:
                        logger.info(f"Skipping entry {data.get('pulse_name', 'unknown')} due to missing verifying key")
                        invalid_detected = True
                        continue
                    entry = LedgerEntry(
                        timestamp=data["timestamp"],
                        pulse_name=data["pulse_name"],
                        operation=data["operation"],
                        details=data["details"],
                        signature=base64.b64decode(data["signature"]),
                        description=desc,
                        verifying_key=verifying_key
                    )
                    self.entries.append(entry)
                except (json.JSONDecodeError, KeyError, ValueError) as e:
                    logger.error(f"Failed to load ledger entry: {e}")
                    invalid_detected = True
        if clear_if_invalid and invalid_detected:
            logger.info(f"Invalid entries detected; clearing {filename}")
            self.entries.clear()
            if os.path.exists(filename):
                os.remove(filename)

# === BCI Adapter ===
class BCIAdapter:
    def __init__(self, eeg_source):
        self.source = eeg_source

    def get_current_state(self) -> Dict[str, Any]:
        return self.source.read_brainwave_data()

    def interpret(self, state: Dict[str, Any]) -> tuple[str, float]:
        alpha = state.get("alpha", 0.0)
        beta = state.get("beta", 0.0)
        if alpha > 0.7:
            return "relaxed", min(1.0, alpha)
        elif beta > 0.7:
            return "focused", min(1.0, beta)
        return "neutral", 0.5

# === Enhanced Fractal Timing System ===
@dataclass
class Signal:
    from_agent: str
    name: str
    time: float
    strength: float = 1.0
    signature: Optional[bytes] = None

@dataclass
class Action:
    type: str
    signal: Optional[str] = None
    to: Optional[Union[str, List[str]]] = None
    phase_shift: Optional[float] = None
    fraction: Optional[int] = None
    condition: Optional[str] = None
    strength: Optional[float] = None

@dataclass
class Pulse:
    name: str
    interval: float
    next_fire: float
    body: List[Action]
    time_scale: float = 1.0
    fractions: int = 1
    enabled: bool = True
    inbox: List[Signal] = field(default_factory=list)
    logger: TCCLogger = field(default_factory=TCCLogger)
    signing_key: nacl.signing.SigningKey = field(default_factory=nacl.signing.SigningKey.generate)
    bci_adapter: Optional[BCIAdapter] = None

    def should_fire(self, global_time: float) -> bool:
        if not self.enabled:
            self.logger.log(
                "pulse_check",
                str(global_time).encode('utf-8'),
                b"disabled",
                {"pulse_name": self.name, "enabled": self.enabled},
                "INFO",
                "PULSE_DISABLED"
            )
            return False

        local_time = global_time * self.time_scale
        should_fire = abs(local_time - self.next_fire) < 1e-6
        self.logger.log(
            "pulse_check",
            str(global_time).encode('utf-8'),
            b"firing" if should_fire else b"not_ready",
            {"pulse_name": self.name, "global_time": global_time, "local_time": local_time, "next_fire": self.next_fire},
            "INFO",
            "PULSE_FIRING" if should_fire else "PULSE_NOT_READY"
        )
        return should_fire

    def on_signal(self, signal: Signal, state: 'State', ledger: TransparencyLedger) -> None:
        if signal.signature:
            try:
                verifying_key = nacl.signing.VerifyKey(
                    self.signing_key.verify_key.encode(encoder=nacl.encoding.RawEncoder)
                )
                verifying_key.verify(
                    f"{signal.from_agent}:{signal.name}:{signal.time}:{signal.strength}".encode('utf-8'),
                    signal.signature
                )
            except nacl.exceptions.BadSignatureError:
                self.logger.log(
                    "signal_verification",
                    signal.name.encode('utf-8'),
                    b"failed",
                    {"from_agent": signal.from_agent, "signal_time": signal.time, "strength": signal.strength},
                    "ERROR",
                    "INVALID_SIGNATURE"
                )
                return

        description = f"Received signal {signal.name} from {signal.from_agent} at time {signal.time} with strength {signal.strength}"
        self.logger.log(
            "signal_received",
            signal.name.encode('utf-8'),
            b"processed",
            {"from_agent": signal.from_agent, "signal_name": signal.name, "signal_time": signal.time, "strength": signal.strength},
            "INFO",
            "SUCCESS"
        )
        ledger.add_entry(
            self.name,
            "signal_received",
            {"from_agent": signal.from_agent, "signal_name": signal.name, "signal_time": signal.time, "strength": signal.strength},
            description
        )

        if signal.name == "sync":
            self.next_fire += signal.strength
            self.logger.log(
                "signal_sync",
                signal.name.encode('utf-8'),
                b"modulated",
                {"pulse_name": self.name, "next_fire": self.next_fire, "strength": signal.strength},
                "INFO",
                "SYNC_APPLIED"
            )
            print(f" Modulated [{self.name}] by +{signal.strength} at t={signal.time}")

        if signal.name == "off":
            self.enabled = False
            self.logger.log(
                "signal_off",
                signal.name.encode('utf-8'),
                b"disabled",
                {"pulse_name": self.name, "enabled": self.enabled, "strength": signal.strength},
                "INFO",
                "PULSE_DISABLED"
            )
            print(f" [{self.name}] switched OFF by signal at t={signal.time} (strength: {signal.strength})")

        if signal.name == "on":
            self.enabled = True
            self.logger.log(
                "signal_on",
                signal.name.encode('utf-8'),
                b"enabled",
                {"pulse_name": self.name, "enabled": self.enabled, "strength": signal.strength},
                "INFO",
                "PULSE_ENABLED"
            )
            print(f" [{self.name}] switched ON by signal at t={signal.time} (strength: {signal.strength})")

    def fire(self, global_time: float, state: 'State', ledger: TransparencyLedger) -> List[Signal]:
        if not self.should_fire(global_time):
            return []

        if self.fractions <= 0:
            self.logger.log(
                "pulse_error",
                str(global_time).encode('utf-8'),
                b"invalid fractions",
                {"pulse_name": self.name, "fractions": self.fractions},
                "ERROR",
                "INVALID_FRACTIONS"
            )
            return []

        print(f" Firing [{self.name}] at t={global_time}")
        self.logger.log(
            "pulse_fire",
            str(global_time).encode('utf-8'),
            b"fired",
            {"pulse_name": self.name, "global_time": global_time},
            "INFO",
            "PULSE_FIRED"
        )

        signals_emitted = []
        local_time = global_time * self.time_scale
        self.next_fire += self.interval

        for f in range(self.fractions):
            for action in self.body:
                if action.fraction is not None and action.fraction != f:
                    continue

                action_strength = min(1.0, max(0.0, action.strength if action.strength is not None else 1.0))
                if action.condition:
                    condition_met, condition_strength = self.evaluate_condition(action.condition)
                    if not condition_met:
                        self.logger.log(
                            "action_skip",
                            action.signal.encode('utf-8') if action.signal else b'',
                            b"condition not met",
                            {"pulse_name": self.name, "condition": action.condition, "strength": action_strength},
                            "INFO",
                            "CONDITION_FAIL"
                        )
                        continue
                    action_strength *= condition_strength

                action_details = {
                    "pulse_name": self.name,
                    "action_type": action.type,
                    "signal": action.signal,
                    "to": action.to,
                    "phase_shift": action.phase_shift,
                    "fraction": action.fraction,
                    "condition": action.condition,
                    "strength": action_strength
                }

                description = f"Performed action: {action.type} signal {action.signal} to {action.to} at time {global_time} with strength {action_strength}"
                ledger.add_entry(self.name, action.type, action_details, description)

                if action.type == "emit" and action.signal:
                    targets = [action.to] if isinstance(action.to, str) else (action.to or [])
                    for target in targets:
                        sig = Signal(
                            from_agent=self.name,
                            name=action.signal,
                            time=global_time,
                            strength=action_strength,
                            signature=self.sign_signal(action.signal, global_time, action_strength)
                        )
                        if target in state.pulses:
                            state.pulses[target].inbox.append(sig)
                            self.logger.log(
                                "action_emit",
                                action.signal.encode('utf-8'),
                                b"emitted",
                                {"target": target, "signal_name": action.signal, "strength": action_strength},
                                "INFO",
                                "ACTION_EMITTED"
                            )
                            print(f" Emitted [{sig.name}] from [{self.name}] to [{target}] at t={global_time} (strength: {action_strength})")
                        signals_emitted.append(sig)
                        state.signals.append(sig)

                elif action.type == "broadcast" and action.signal:
                    for target in state.pulses:
                        if target != self.name:
                            sig = Signal(
                                from_agent=self.name,
                                name=action.signal,
                                time=global_time,
                                strength=action_strength,
                                signature=self.sign_signal(action.signal, global_time, action_strength)
                            )
                            state.pulses[target].inbox.append(sig)
                            self.logger.log(
                                "action_broadcast",
                                action.signal.encode('utf-8'),
                                b"broadcasted",
                                {"target": target, "signal_name": action.signal, "strength": action_strength},
                                "INFO",
                                "ACTION_BROADCASTED"
                            )
                            print(f" Broadcasted [{action.signal}] from [{self.name}] to [{target}] at t={global_time} (strength: {action_strength})")
                            signals_emitted.append(sig)
                            state.signals.append(sig)

                elif action.type == "modulate" and action.signal and action.phase_shift is not None:
                    if action.to in state.pulses:
                        target_pulse = state.pulses[action.to]
                        target_pulse.next_fire += action.phase_shift * action_strength
                        self.logger.log(
                            "action_modulate",
                            action.signal.encode('utf-8'),
                            b"modulated",
                            {"target": action.to, "phase_shift": action.phase_shift, "strength": action_strength},
                            "INFO",
                            "ACTION_MODULATED"
                        )
                        print(f" Modulated [{action.to}] by {action.phase_shift * action_strength} from [{self.name}] at t={global_time}")

                elif action.type == "modulate_time" and action.fraction is not None:
                    if action.fraction == 0:
                        self.logger.log(
                            "action_modulate_time_error",
                            str(self.time_scale).encode('utf-8'),
                            b"invalid fraction",
                            {"pulse_name": self.name, "fraction": action.fraction},
                            "ERROR",
                            "INVALID_FRACTION"
                        )
                        continue
                    old_time_scale = self.time_scale
                    self.time_scale = max(0.1, min(self.time_scale / action.fraction * action_strength, 10.0))
                    self.logger.log(
                        "action_modulate_time",
                        str(old_time_scale).encode('utf-8'),
                        str(self.time_scale).encode('utf-8'),
                        {"pulse_name": self.name, "fraction": action.fraction, "new_time_scale": self.time_scale, "strength": action_strength},
                        "INFO",
                        "ACTION_MODULATED_TIME"
                    )
                    print(f" [{self.name}] time_scale changed from {old_time_scale} to {self.time_scale} at t={global_time} (strength: {action_strength})")

        for signal in self.inbox:
            self.on_signal(signal, state, ledger)
        self.inbox.clear()

        return signals_emitted

    def evaluate_condition(self, condition: str) -> tuple[bool, float]:
        if self.bci_adapter:
            brain_state, strength = self.bci_adapter.interpret(self.bci_adapter.get_current_state())
            return brain_state == condition, strength
        return True, 1.0

    def sign_signal(self, signal_name: str, timestamp: float, strength: float) -> bytes:
        msg = f"{self.name}:{signal_name}:{timestamp}:{strength}".encode('utf-8')
        return self.signing_key.sign(msg).signature

    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "interval": self.interval,
            "next_fire": self.next_fire,
            "body": [{"type": a.type, "signal": a.signal, "to": a.to, "phase_shift": a.phase_shift,
                      "fraction": a.fraction, "condition": a.condition, "strength": a.strength} for a in self.body],
            "time_scale": self.time_scale,
            "fractions": self.fractions,
            "enabled": self.enabled
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Pulse':
        return cls(
            name=data["name"],
            interval=data["interval"],
            next_fire=data["next_fire"],
            body=[Action(**action) for action in data["body"]],
            time_scale=data.get("time_scale", 1.0),
            fractions=data.get("fractions", 1),
            enabled=data.get("enabled", True)
        )

# === Quantum Pulse ===
class QuantumPulse(Pulse):
    def __init__(self, *args, state_vector: Optional[Dict[str, float]] = None, **kwargs):
        super().__init__(*args, **kwargs)
        self.state_vector = state_vector or {"on": 0.5, "off": 0.5}

    def should_fire(self, global_time: float) -> bool:
        if not self.enabled:
            self.logger.log(
                "pulse_check",
                str(global_time).encode('utf-8'),
                b"disabled",
                {"pulse_name": self.name, "enabled": self.enabled},
                "INFO",
                "PULSE_DISABLED"
            )
            return False

        collapsed_state, strength = self.collapse_state()
        should_fire = collapsed_state == "on"
        self.logger.log(
            "pulse_check",
            str(global_time).encode('utf-8'),
            b"firing" if should_fire else b"not_firing",
            {"pulse_name": self.name, "global_time": global_time, "state": collapsed_state, "strength": strength},
            "INFO",
            "PULSE_FIRING" if should_fire else "PULSE_NOT_READY"
        )
        return should_fire

    def collapse_state(self) -> tuple[str, float]:
        rand = random.random()
        cumulative = 0.0
        for state, prob in self.state_vector.items():
            cumulative += prob
            if rand <= cumulative:
                strength = prob if state == "on" else 1.0 - prob
                return state, min(1.0, max(0.0, strength))
        return "off", 0.5

@dataclass
class State:
    time: float = 0.0
    pulses: Dict[str, Any] = field(default_factory=dict)
    signals: List[Signal] = field(default_factory=list)

    def save_state(self, filename: str) -> None:
        state_data = {
            "time": self.time,
            "pulses": {name: pulse.to_dict() for name, pulse in self.pulses.items()},
            "signals": [{"from_agent": s.from_agent, "name": s.name, "time": s.time, "strength": s.strength,
                         "signature": base64.b64encode(s.signature).decode('utf-8') if s.signature else None}
                        for s in self.signals]
        }
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(state_data, f, indent=2)

    def load_state(self, filename: str) -> None:
        if not os.path.exists(filename):
            return
        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.time = data.get("time", 0.0)
            self.pulses = {name: Pulse.from_dict(pulse_data) for name, pulse_data in data.get("pulses", {}).items()}
            self.signals = [
                Signal(
                    from_agent=s["from_agent"],
                    name=s["name"],
                    time=s["time"],
                    strength=s.get("strength", 1.0),
                    signature=base64.b64decode(s["signature"]) if s["signature"] else None
                ) for s in data.get("signals", [])
            ]

# === Pulse Configuration Loader ===
def load_pulse_config(filename: str) -> List[Pulse]:
    if not os.path.exists(filename):
        return []
    with open(filename, 'r', encoding='utf-8') as f:
        config = json.load(f)
        pulses = []
        for p in config.get("pulses", []):
            if p.get("is_quantum", False):
                pulses.append(QuantumPulse(
                    name=p["name"],
                    interval=p["interval"],
                    next_fire=p["next_fire"],
                    body=[Action(**action) for action in p["body"]],
                    time_scale=p.get("time_scale", 1.0),
                    fractions=p.get("fractions", 1),
                    enabled=p.get("enabled", True),
                    state_vector=p.get("state_vector", {"on": 0.5, "off": 0.5})
                ))
            else:
                pulses.append(Pulse(
                    name=p["name"],
                    interval=p["interval"],
                    next_fire=p["next_fire"],
                    body=[Action(**action) for action in p["body"]],
                    time_scale=p.get("time_scale", 1.0),
                    fractions=p.get("fractions", 1),
                    enabled=p.get("enabled", True)
                ))
        return pulses

# === Mock EEG Source ===
class MockEEGSource:
    def read_brainwave_data(self) -> Dict[str, Any]:
        return {
            "alpha": random.uniform(0.0, 1.0),
            "beta": random.uniform(0.0, 1.0)
        }

# === Example Usage ===
if __name__ == "__main__":
    random.seed(42)  # For reproducibility
    ledger = TransparencyLedger()
    state = State()

    # Initialize BCI adapter with mock EEG source
    eeg_source = MockEEGSource()
    bci_adapter = BCIAdapter(eeg_source)

    # Load state if available
    state.load_state("state.json")
    ledger.load_ledger("ledger.json", clear_if_invalid=True)

    # Load pulse configuration from JSON file if available, else use default
    config_file = "pulse_config.json"
    if os.path.exists(config_file):
        pulses = load_pulse_config(config_file)
        for pulse in pulses:
            if any(action.condition for action in pulse.body):
                pulse.bci_adapter = bci_adapter
            state.pulses[pulse.name] = pulse
    else:
        state.pulses["alpha"] = Pulse(
            name="alpha",
            interval=4.0,
            next_fire=4.0,
            fractions=2,
            body=[
                Action(type="emit", signal="sync", to="beta", fraction=0, condition="relaxed", strength=0.8),
                Action(type="modulate_time", fraction=2, to="gamma", strength=0.9)
            ],
            bci_adapter=bci_adapter
        )
        state.pulses["beta"] = QuantumPulse(
            name="beta",
            interval=3.0,
            next_fire=3.0,
            fractions=1,
            body=[
                Action(type="emit", signal="beta_ping", to="gamma", condition="focused", strength=0.7),
                Action(type="emit", signal="off", to="gamma", fraction=0, strength=1.0)
            ],
            state_vector={"on": 0.7, "off": 0.3},
            bci_adapter=bci_adapter
        )
        state.pulses["gamma"] = Pulse(
            name="gamma",
            interval=5.0,
            next_fire=5.0,
            fractions=1,
            body=[
                Action(type="broadcast", signal="gamma_wave", condition="neutral", strength=0.6)
            ],
            bci_adapter=bci_adapter
        )
        # Save default configuration
        config = {
            "pulses": [
                {**p.to_dict(), "is_quantum": isinstance(p, QuantumPulse), "state_vector": getattr(p, "state_vector", None)}
                for p in state.pulses.values()
            ]
        }
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2)

    state.time = 0.0
    while state.time <= 20.0:
        for name in list(state.pulses.keys()):
            pulse = state.pulses[name]
            signals = pulse.fire(state.time, state, ledger)
            for sig in signals:
                description = f"Signal {sig.name} emitted from {pulse.name} at time {sig.time} with strength {sig.strength}"
                ledger.add_entry(pulse.name, "fire", {"signal": sig.name, "time": sig.time, "strength": sig.strength}, description)
        state.time += 1.0

    # Save state and logs
    state.save_state("state.json")
    for pulse in state.pulses.values():
        pulse.logger.save_log(f"{pulse.name}_log.json")
    ledger.save_ledger("ledger.json")

    print("\n=== Ledger Verification ===")
    for entry in ledger.entries:
        print(f"Ledger entry for {entry.pulse_name} ({entry.operation}) verified: {ledger.verify_entry(entry)}")

    print("\n=== Final Signals ===")
    for sig in sorted(state.signals, key=lambda x: x.time):
        print(f" {sig.from_agent} -> {sig.name} at t={sig.time} (strength: {sig.strength})")

import json
import time
import hashlib
import base64
import nacl.signing
from typing import Dict, List, Any
from mytech import TCCLogger, TransparencyLedger, QuantumPulse, LLMFlow

class RSLInterpreter:
    def __init__(self, ledger: TransparencyLedger, pulse: QuantumPulse, flow: LLMFlow):
        self.ledger = ledger
        self.pulse = pulse
        self.flow = flow
        self.logger = TCCLogger()
        self.namespaces = {}
        self.sids = {}  # Store soulbound identities

    def create_sid(self, agent_name: str, agent_type: str) -> Dict[str, Any]:
        """Create a soulbound identity (SID) for an agent."""
        signing_key = nacl.signing.SigningKey.generate()
        sid = hashlib.sha256(f"{agent_name}:{time.time_ns()}".encode()).hexdigest()
        sid_data = {
            "sid": sid,
            "public_key": base64.b64encode(signing_key.verify_key.encode()).decode(),
            "metadata": {"type": agent_type, "created": time.time_ns()}
        }
        self.ledger.add_entry("sid_creation", "create", sid_data, f"Created SID for {agent_name}")
        self.sids[sid] = sid_data
        return sid_data

    def parse_dsl(self, dsl_program: str) -> Dict[str, Any]:
        """Parse DSL program into an AST (simplified for JSON-based DSL)."""
        try:
            ast = json.loads(dsl_program)
            self.logger.log("parse_dsl", dsl_program.encode(), b"", {"status": "parsed"})
            return ast
        except json.JSONDecodeError as e:
            self.logger.log("parse_error", dsl_program.encode(), str(e).encode(), {"status": "failed"})
            raise ValueError(f"Invalid DSL program: {e}")

    def execute_program(self, dsl_program: str, sid: str) -> Dict[str, Any]:
        """Execute a DSL program, enforcing QPES rules."""
        ast = self.parse_dsl(dsl_program)
        if ast["program"].get("namespace"):
            return self.execute_namespace(ast["program"]["namespace"], sid)
        elif ast["program"].get("promise-decl"):
            return self.execute_promise(ast["program"]["promise-decl"], sid)
        elif ast["program"].get("consensus-decl"):
            return self.execute_consensus(ast["program"]["consensus-decl"], sid)
        else:
            raise ValueError("Unsupported program structure")

    def execute_namespace(self, namespace: Dict[str, Any], sid: str) -> Dict[str, Any]:
        """Execute a namespace declaration, creating a scope."""
        name = namespace["name"]
        self.namespaces[name] = {"packs": {}, "things": {}}
        self.ledger.add_entry("namespace_create", "create", {"sid": sid, "name": name}, f"Namespace {name} created")
        for pack in namespace.get("pack", []):
            self.namespaces[name]["packs"][pack["name"]] = pack
        for thing in namespace.get("thing", []):
            self.namespaces[name]["things"][thing["name"]] = thing
        return {"status": "namespace_created", "name": name}

    def execute_promise(self, promise: Dict[str, Any], sid: str) -> Dict[str, Any]:
        """Execute a promise declaration, creating a covenant (QPES rule 9)."""
        name = promise["name"]
        roles = promise.get("needs", {}).get("role-list", [])
        jobs = promise.get("binds", {}).get("job-list", [])
        check_expr = promise.get("check", {})
        enforce_actions = promise.get("enforce", [])

        # QPES Rule 9: Bond agents via covenant
        covenant = {"sid": sid, "name": name, "roles": roles, "jobs": jobs}
        self.ledger.add_entry("promise_create", "bond", covenant, f"Covenant {name} created by {sid}")

        # QPES Rule 4: Alignment overrides entropy
        if check_expr:
            coherence = self.compute_coherence(check_expr, sid)
            if coherence < 0.7:  # Threshold from QPES rule 8
                self.logger.log("promise_reject", json.dumps(covenant).encode(), b"", {"reason": "low coherence"})
                return {"status": "rejected", "reason": "Insufficient coherence"}

        # Execute enforcement actions
        for action in enforce_actions:
            self.execute_action(action, sid)
        return {"status": "promise_bound", "name": name}

    def execute_consensus(self, consensus: Dict[str, Any], sid: str) -> Dict[str, Any]:
        """Execute a consensus declaration, enforcing shared collapse (QPES rule 8)."""
        name = consensus["name"]
        threshold = consensus["threshold"]
        voters = consensus["voters"]
        actions = consensus["actions"]

        # QPES Rule 8: Shared collapse requires threshold coherence
        coherence = self.compute_coherence_voters(voters, sid)
        if coherence < threshold:
            self.logger.log("consensus_reject", json.dumps(consensus).encode(), b"", {"reason": "low coherence"})
            return {"status": "rejected", "reason": f"Coherence {coherence} below threshold {threshold}"}

        # QPES Rule 1 & 2: Superposition and intentional collapse
        self.pulse.state_vector = {f"vote_{v}": 1.0 / len(voters) for v in voters}
        state, strength = self.pulse.collapse_state()
        self.ledger.add_entry("consensus_collapse", "collapse", {
            "sid": sid, "name": name, "state": state, "strength": strength
        }, f"Consensus {name} collapsed to {state}")

        # Execute actions
        for action in actions:
            self.execute_action(action, sid)
        return {"status": "consensus_reached", "name": name, "state": state}

    def compute_coherence(self, expr: Dict[str, Any], sid: str) -> float:
        """Compute coherence for an expression using LLMFlow (QPES rule 5)."""
        intent = json.dumps(expr)
        result = self.flow.execute(intent)
        coherence = float(result.get("output_text", 0.5))  # Simplified coherence score
        self.logger.log("coherence_compute", intent.encode(), str(coherence).encode(), {"sid": sid})
        return coherence

    def compute_coherence_voters(self, voters: List[str], sid: str) -> float:
        """Compute coherence across voters (QPES rule 8)."""
        intents = [self.sids[v]["metadata"].get("intent", "") for v in voters if v in self.sids]
        if not intents:
            return 0.0
        coherence = sum(self.flow.execute(f"{i1}:{i2}")["output_text"] for i1, i2 in zip(intents, intents[1:])) / len(intents)
        self.logger.log("voter_coherence", json.dumps(voters).encode(), str(coherence).encode(), {"sid": sid})
        return float(coherence)

    def execute_action(self, action: Dict[str, Any], sid: str) -> None:
        """Execute a DSL action, logging to TransparencyLedger (QPES rule 7)."""
        action_type = action.get("type")
        self.ledger.add_entry(f"action_{action_type}", action_type, {"sid": sid, "action": action}, f"Action {action_type} by {sid}")
        # Simplified action execution (extend for specific actions like set-action, if-action, etc.)
        self.logger.log(f"action_{action_type}", json.dumps(action).encode(), b"", {"sid": sid})

# Example usage
if __name__ == "__main__":
    ledger = TransparencyLedger()
    pulse = QuantumPulse(name="system", interval=1.0, next_fire=1.0, state_vector={"active": 0.5, "inactive": 0.5})
    flow = define_default_flow()  # From mytech.txt
    interpreter = RSLInterpreter(ledger, pulse, flow)

    # Create a soulbound identity
    sid = interpreter.create_sid("alice", "human")["sid"]

    # Example DSL program (promise-decl)
    dsl_program = '''
    {
        "program": {
            "promise-decl": {
                "name": "sync_covenant",
                "needs": {"role-list": ["role admin", "role user"]},
                "binds": {"job-list": ["job sync_task"]},
                "check": {"expr": "admin.intent == user.intent"},
                "enforce": [{"type": "set-action", "name": "state", "expr": "sync"}]
            }
        }
    }
    '''
    result = interpreter.execute_program(dsl_program, sid)
    print(result)

#!/usr/bin/env python3
"""
Dual License

For Open-Source Individuals:
MIT License

Copyright (c) 2025 James B. Chapman

Permission is hereby granted, free of charge, to any individual obtaining a copy
of this software and associated documentation files (the "Software"), for personal,
non-commercial use, to deal in the Software without restriction, including without
limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,
and/or sell copies of the Software, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

For Companies:
Commercial use by companies requires a separate license. Contact iconoclastdao@gmail.com
for licensing terms and conditions. Unauthorized commercial use is prohibited.
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Union, Any
import json
import time
import hashlib
import base64
import struct
import nacl.signing
import nacl.encoding
import nacl.exceptions
import logging
import os
import random
from pathlib import Path

# Setup logging
AETHER_DIR = Path("/Users/jameschapman/AetherGuide/sonrev_stack")
LOG_DIR = AETHER_DIR / "logs"
DATA_DIR = AETHER_DIR / "data"
LOG_FILE = LOG_DIR / "sovereign_stack.log"
CHAIN_FILE = DATA_DIR / "chain.json"
LEDGER_FILE = DATA_DIR / "ledger.json"
STATE_FILE = DATA_DIR / "state.json"
CONFIG_FILE = DATA_DIR / "pulse_config.json"

for dir_path in [LOG_DIR, DATA_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# === TCC Logger and Log Entry ===
class TCCLogger:
    def __init__(self):
        self.tcc_log: List[TCCLogEntry] = []
        self.step_counter: int = 0
        self.signing_key = nacl.signing.SigningKey.generate()
        self.verifying_key = self.signing_key.verify_key

    def log(self, operation: str, input_data: bytes, output_data: bytes,
            metadata: Dict[str, Any] = None, log_level: str = "INFO", error_code: str = "NONE") -> None:
        entry = TCCLogEntry(
            step=self.step_counter,
            operation=operation,
            input_data=input_data,
            output_data=output_data,
            metadata=metadata or {},
            log_level=log_level,
            error_code=error_code,
            prev_hash=self._compute_prev_hash(),
            signing_key=self.signing_key
        )
        self.tcc_log.append(entry)
        self.step_counter += 1

    def _compute_prev_hash(self) -> bytes:
        if not self.tcc_log:
            return b'\x00' * 32
        last_entry = self.tcc_log[-1]
        return hashlib.sha256(last_entry.to_bytes()).digest()

    def save_log(self, filename: str) -> None:
        with open(filename, 'w', encoding='utf-8', errors='replace') as f:
            for entry in self.tcc_log:
                f.write(json.dumps(entry.to_json()) + '\n')

class TCCLogEntry:
    def __init__(self, step: int, operation: str, input_data: bytes, output_data: bytes,
                 metadata: Dict[str, Any], log_level: str, error_code: str, prev_hash: bytes,
                 signing_key: nacl.signing.SigningKey):
        self.step = step
        self.operation = operation
        self.input_data = input_data
        self.output_data = output_data
        self.metadata = metadata
        self.log_level = log_level
        self.error_code = error_code
        self.prev_hash = prev_hash
        self.operation_id = hashlib.sha256(f"{step}:{operation}:{time.time_ns()}".encode()).hexdigest()[:32]
        self.timestamp = time.time_ns()
        self.execution_time_ns = 0
        self.signature = b''
        entry_bytes = self._to_bytes_without_signature()
        self.signature = signing_key.sign(entry_bytes).signature

    def _to_bytes_without_signature(self) -> bytes:
        step_bytes = struct.pack('>Q', self.step)
        op_bytes = self.operation.encode('utf-8').ljust(32, b'\x00')[:32]
        input_len_bytes = struct.pack('>I', len(self.input_data))
        output_len_bytes = struct.pack('>I', len(self.output_data))
        meta_bytes = json.dumps(self.metadata).encode('utf-8').ljust(128, b'\x00')[:128]
        level_bytes = self.log_level.encode('utf-8').ljust(16, b'\x00')[:16]
        error_bytes = self.error_code.encode('utf-8').ljust(16, b'\x00')[:16]
        op_id_bytes = self.operation_id.encode('utf-8').ljust(32, b'\x00')[:32]
        ts_bytes = struct.pack('>q', self.timestamp)
        exec_time_bytes = struct.pack('>q', self.execution_time_ns)
        return (
            step_bytes + op_bytes + input_len_bytes + self.input_data +
            output_len_bytes + self.output_data + meta_bytes + level_bytes +
            error_bytes + self.prev_hash + op_id_bytes + ts_bytes + exec_time_bytes
        )

    def to_bytes(self) -> bytes:
        start_time = time.time_ns()
        result = self._to_bytes_without_signature() + self.signature
        self.execution_time_ns = time.time_ns() - start_time
        return result

    def to_json(self) -> Dict[str, Any]:
        return {
            "step": str(self.step),
            "operation": self.operation,
            "input_data": base64.b64encode(self.input_data).decode('utf-8'),
            "output_data": base64.b64encode(self.output_data).decode('utf-8'),
            "metadata": self.metadata,
            "log_level": self.log_level,
            "error_code": self.error_code,
            "prev_hash": base64.b64encode(self.prev_hash).decode('utf-8'),
            "operation_id": self.operation_id,
            "timestamp": str(self.timestamp),
            "execution_time_ns": str(self.execution_time_ns),
            "signature": base64.b64encode(self.signature).decode('utf-8')
        }

# === Transparency Ledger ===
@dataclass
class LedgerEntry:
    timestamp: float
    pulse_name: str
    operation: str
    details: Dict[str, Any]
    signature: bytes
    description: str
    verifying_key: Optional[bytes] = None

class TransparencyLedger:
    def __init__(self):
        self.entries: List[LedgerEntry] = []
        self.logger = TCCLogger()
        self.signing_key = nacl.signing.SigningKey.generate()
        self.verifying_key = self.signing_key.verify_key

    def add_entry(self, pulse_name: str, operation: str, details: Dict[str, Any], description: str) -> None:
        timestamp = time.time()
        details_bytes = json.dumps(details, sort_keys=True).encode('utf-8')
        signature = self.signing_key.sign(details_bytes).signature
        entry = LedgerEntry(
            timestamp=timestamp,
            pulse_name=pulse_name,
            operation=operation,
            details=details,
            signature=signature,
            description=description,
            verifying_key=self.verifying_key.encode(encoder=nacl.encoding.RawEncoder)
        )
        self.entries.append(entry)
        self.logger.log(
            "ledger_entry",
            details_bytes,
            signature,
            {"pulse_name": pulse_name, "operation": operation, "timestamp": timestamp},
            "INFO",
            "SUCCESS"
        )

    def verify_entry(self, entry: LedgerEntry) -> bool:
        if not entry.verifying_key or len(entry.verifying_key) != 32:
            logger.info(f"Legacy or invalid entry skipped for {entry.pulse_name} ({entry.operation})")
            return False
        try:
            details_bytes = json.dumps(entry.details, sort_keys=True).encode('utf-8')
            verifying_key = nacl.signing.VerifyKey(entry.verifying_key)
            verifying_key.verify(details_bytes, entry.signature)
            return True
        except (nacl.exceptions.BadSignatureError, nacl.exceptions.ValueError):
            logger.warning(f"Signature verification failed for entry {entry.pulse_name} ({entry.operation})")
            return False

    def save_ledger(self, filename: str) -> None:
        with open(filename, 'w', encoding='utf-8', errors='replace') as f:
            for entry in self.entries:
                f.write(json.dumps({
                    "timestamp": entry.timestamp,
                    "pulse_name": entry.pulse_name,
                    "operation": entry.operation,
                    "details": entry.details,
                    "signature": base64.b64encode(entry.signature).decode('utf-8'),
                    "description": entry.description,
                    "verifying_key": base64.b64encode(entry.verifying_key).decode('utf-8') if entry.verifying_key else ""
                }) + '\n')

    def load_ledger(self, filename: str, clear_if_invalid: bool = False) -> None:
        if not os.path.exists(filename):
            return
        invalid_detected = False
        self.entries.clear()
        with open(filename, 'r', encoding='utf-8', errors='replace') as f:
            for line in f:
                try:
                    data = json.loads(line.strip())
                    desc = data.get("description", data.get("nl_description", "No description available"))
                    verifying_key = None
                    if "verifying_key" in data and data["verifying_key"]:
                        try:
                            vk = base64.b64decode(data["verifying_key"])
                            if len(vk) == 32:
                                verifying_key = vk
                            else:
                                logger.info(f"Skipping entry {data.get('pulse_name', 'unknown')} due to invalid verifying key length")
                                invalid_detected = True
                                continue
                        except ValueError:
                            logger.info(f"Skipping entry {data.get('pulse_name', 'unknown')} due to invalid verifying key decoding")
                            invalid_detected = True
                            continue
                    if not verifying_key:
                        logger.info(f"Skipping entry {data.get('pulse_name', 'unknown')} due to missing verifying key")
                        invalid_detected = True
                        continue
                    entry = LedgerEntry(
                        timestamp=data["timestamp"],
                        pulse_name=data["pulse_name"],
                        operation=data["operation"],
                        details=data["details"],
                        signature=base64.b64decode(data["signature"]),
                        description=desc,
                        verifying_key=verifying_key
                    )
                    self.entries.append(entry)
                except (json.JSONDecodeError, KeyError, ValueError) as e:
                    logger.error(f"Failed to load ledger entry: {e}")
                    invalid_detected = True
        if clear_if_invalid and invalid_detected:
            logger.info(f"Invalid entries detected; clearing {filename}")
            self.entries.clear()
            if os.path.exists(filename):
                os.remove(filename)

# === BCI Adapter ===
class BCIAdapter:
    def __init__(self, eeg_source):
        self.source = eeg_source

    def get_current_state(self) -> Dict[str, Any]:
        return self.source.read_brainwave_data()

    def interpret(self, state: Dict[str, Any]) -> tuple[str, float]:
        alpha = state.get("alpha", 0.0)
        beta = state.get("beta", 0.0)
        if alpha > 0.7:
            return "relaxed", min(1.0, alpha)
        elif beta > 0.7:
            return "focused", min(1.0, beta)
        return "neutral", 0.5

# === Quantum Pulse Emulation Stack DSL Interpreter ===
class QPulseDSLInterpreter:
    def __init__(self, state: 'State', ledger: TransparencyLedger):
        self.state = state
        self.ledger = ledger
        self.nodes = {}  # node_name: {states: List[str], current: Optional[str]}

    def execute(self, dsl_command: str) -> Any:
        parts = dsl_command.strip().split()
        if not parts:
            raise ValueError("Empty DSL command")

        command = parts[0].lower()
        if command == "node":
            return self._handle_node(parts[1:])
        elif command == "collapse":
            return self._handle_collapse(parts[1:])
        elif command == "emit":
            return self._handle_emit(parts[1:])
        elif command == "resonance":
            return self._handle_resonance(parts[1:])
        elif command == "bond":
            return self._handle_bond(parts[1:])
        elif command == "verify":
            return self._handle_verify(parts[1:])
        else:
            raise ValueError(f"Unknown DSL command: {command}")

    def _handle_node(self, parts: List[str]) -> None:
        if len(parts) < 2 or parts[1] != "|":
            raise ValueError("Invalid node syntax")
        node_name = parts[0]
        states = [s.strip("[]") for s in parts[3:] if s.strip("[]")]
        self.nodes[node_name] = {"states": states, "current": None}
        self.ledger.add_entry(
            "system", "node_created",
            {"node_name": node_name, "states": states},
            f"Created node {node_name} with states {states}"
        )

    def _handle_collapse(self, parts: List[str]) -> str:
        if len(parts) < 3 or parts[1].lower() != "when":
            raise ValueError("Invalid collapse syntax")
        node_name = parts[0]
        if node_name not in self.nodes:
            raise ValueError(f"Node {node_name} not found")
        
        intent = None
        target_state = None
        for i, part in enumerate(parts):
            if part == "intent" and i + 2 < len(parts) and parts[i+1] == "==":
                intent = parts[i+2].strip('"')
            if part == "->" and i + 1 < len(parts):
                target_state = parts[i+1]
        
        node = self.nodes[node_name]
        if intent == "align" and target_state in node["states"]:
            node["current"] = target_state
        else:
            node["current"] = node["states"][0]  # Default to first state
        self.ledger.add_entry(
            "system", "node_collapsed",
            {"node_name": node_name, "state": node["current"], "intent": intent},
            f"Collapsed node {node_name} to state {node['current']} with intent {intent}"
        )
        return node["current"]

    def _handle_emit(self, parts: List[str]) -> None:
        if len(parts) < 5 or parts[1] != "->" or parts[3] != ":":
            raise ValueError("Invalid emit syntax")
        source = parts[0]
        target = parts[2]
        signal = parts[4]
        time_spec = parts[6] if len(parts) > 6 and parts[5] == "@" else str(time.time())
        if source not in self.state.pulses or target not in self.state.pulses:
            raise ValueError(f"Invalid source or target: {source} -> {target}")
        sig = Signal(
            from_agent=source,
            name=signal,
            time=float(time_spec.split("=")[1].split("")[0]) if "=" in time_spec else time.time(),
            strength=1.0,
            signature=self.state.pulses[source].sign_signal(signal, time.time(), 1.0)
        )
        self.state.pulses[target].inbox.append(sig)
        self.state.signals.append(sig)
        self.ledger.add_entry(
            source, "emit",
            {"target": target, "signal": signal, "time": sig.time},
            f"Emitted {signal} from {source} to {target} at t={sig.time}"
        )

    def _handle_resonance(self, parts: List[str]) -> float:
        if len(parts) < 3 or parts[1] != "=":
            raise ValueError("Invalid resonance syntax")
        intents = [part.strip("()").split(".")[0] for part in parts[2:] if ".intent" in part]
        coherence = self._compute_coherence(intents)
        if "collapse" in parts and coherence > 0.7:
            node_name = parts[parts.index("collapse") + 2]
            self._handle_collapse(["collapse", node_name, "when", "intent", "==", '"align"', "->", "sync"])
        self.ledger.add_entry(
            "system", "resonance_computed",
            {"intents": intents, "coherence": coherence},
            f"Computed resonance for intents {intents}: {coherence}"
        )
        return coherence

    def _handle_bond(self, parts: List[str]) -> None:
        if len(parts) < 5 or parts[1] != "<->" or parts[3] != "via":
            raise ValueError("Invalid bond syntax")
        node1, node2 = parts[0], parts[2]
        covenant = parts[4].strip('"')
        self.ledger.add_entry(
            "system", "bond_created",
            {"node1": node1, "node2": node2, "covenant": covenant},
            f"Bonded {node1} and {node2} via covenant {covenant}"
        )

    def _handle_verify(self, parts: List[str]) -> bool:
        if len(parts) < 2 or parts[1] != "emit":
            raise ValueError("Invalid verify syntax")
        emit_cmd = " ".join(parts[1:]).split("==")[0].strip()
        try:
            self._handle_emit(emit_cmd.split()[1:])
            return True
        except Exception:
            return False

    def _compute_coherence(self, intents: List[str]) -> float:
        if len(intents) < 2:
            return 0.0
        # Simulate coherence as average similarity
        return min(1.0, sum(1.0 / len(intents) for _ in range(len(intents) - 1)) + 0.5)

# === Signal, Action, Pulse, QuantumPulse, State ===
@dataclass
class Signal:
    from_agent: str
    name: str
    time: float
    strength: float = 1.0
    signature: Optional[bytes] = None

@dataclass
class Action:
    type: str
    signal: Optional[str] = None
    to: Optional[Union[str, List[str]]] = None
    phase_shift: Optional[float] = None
    fraction: Optional[int] = None
    condition: Optional[str] = None
    strength: Optional[float] = None

@dataclass
class Pulse:
    name: str
    interval: float
    next_fire: float
    body: List[Action]
    time_scale: float = 1.0
    fractions: int = 1
    enabled: bool = True
    inbox: List[Signal] = field(default_factory=list)
    logger: TCCLogger = field(default_factory=TCCLogger)
    signing_key: nacl.signing.SigningKey = field(default_factory=nacl.signing.SigningKey.generate)
    bci_adapter: Optional[BCIAdapter] = None

    def should_fire(self, global_time: float) -> bool:
        if not self.enabled:
            self.logger.log(
                "pulse_check",
                str(global_time).encode('utf-8'),
                b"disabled",
                {"pulse_name": self.name, "enabled": self.enabled},
                "INFO",
                "PULSE_DISABLED"
            )
            return False
        local_time = global_time * self.time_scale
        should_fire = abs(local_time - self.next_fire) < 1e-6
        self.logger.log(
            "pulse_check",
            str(global_time).encode('utf-8'),
            b"firing" if should_fire else b"not_ready",
            {"pulse_name": self.name, "global_time": global_time, "local_time": local_time, "next_fire": self.next_fire},
            "INFO",
            "PULSE_FIRING" if should_fire else "PULSE_NOT_READY"
        )
        return should_fire

    def on_signal(self, signal: Signal, state: 'State', ledger: TransparencyLedger) -> None:
        if signal.signature:
            try:
                verifying_key = nacl.signing.VerifyKey(
                    self.signing_key.verify_key.encode(encoder=nacl.encoding.RawEncoder)
                )
                verifying_key.verify(
                    f"{signal.from_agent}:{signal.name}:{signal.time}:{signal.strength}".encode('utf-8'),
                    signal.signature
                )
            except nacl.exceptions.BadSignatureError:
                self.logger.log(
                    "signal_verification",
                    signal.name.encode('utf-8'),
                    b"failed",
                    {"from_agent": signal.from_agent, "signal_time": signal.time, "strength": signal.strength},
                    "ERROR",
                    "INVALID_SIGNATURE"
                )
                return
        description = f"Received signal {signal.name} from {signal.from_agent} at time {signal.time} with strength {signal.strength}"
        self.logger.log(
            "signal_received",
            signal.name.encode('utf-8'),
            b"processed",
            {"from_agent": signal.from_agent, "signal_name": signal.name, "signal_time": signal.time, "strength": signal.strength},
            "INFO",
            "SUCCESS"
        )
        ledger.add_entry(
            self.name, "signal_received",
            {"from_agent": signal.from_agent, "signal": signal.name, "time": signal.time, "strength": signal.strength},
            description
        )
        if signal.name == "sync":
            self.next_fire += signal.strength
            print(f" Modulated [{self.name}] by +{signal.strength} at t={signal.time}")
        elif signal.name == "off":
            self.enabled = False
            print(f" [{self.name}] switched OFF by signal at t={signal.time} (strength: {signal.strength})")
        elif signal.name == "on":
            self.enabled = True
            print(f" [{self.name}] switched ON by signal at t={signal.time} (strength: {signal.strength})")

    def fire(self, global_time: float, state: 'State', ledger: TransparencyLedger) -> List[Signal]:
        if not self.should_fire(global_time):
            return []
        if self.fractions <= 0:
            self.logger.log(
                "pulse_error",
                str(global_time).encode(),
                b"invalid fractions",
                {"pulse_name": self.name, "fractions": self.fractions},
                "ERROR",
                "INVALID_FRACTIONS"
            )
            return []
        print(f" Firing [{self.name}] at t={global_time}")
        signals_emitted = []
        local_time = global_time * self.time_scale
        self.next_fire += self.interval
        for f in range(self.fractions):
            for action in self.body:
                if action.fraction is not None and action.fraction != f:
                    continue
                action_strength = min(1.0, max(0.0, action.strength if action.strength else 1.0))
                if action.condition:
                    condition_met, condition_strength = self.evaluate_condition(action.condition)
                    if not condition_met:
                        continue
                    action_strength *= condition_strength
                action_details = {
                    "pulse_name": self.name,
                    "action_type": action.type,
                    "signal": action.signal,
                    "to": action.to,
                    "phase_shift": action.phase_shift,
                    "fraction": action.fraction,
                    "condition": action.condition,
                    "strength": action_strength
                }
                description = f"Performed action: {action.type} signal {action.signal} to {action.to} at time {global_time} with strength {action_strength}"
                ledger.add_entry(self.name, action.type, action_details, description)
                if action.type == "emit" and action.signal:
                    targets = [action.to] if isinstance(action.to, str) else (action.to or [])
                    for target in targets:
                        sig = Signal(
                            from_agent=self.name,
                            name=action.signal,
                            time=global_time,
                            strength=action_strength,
                            signature=self.sign_signal(action.signal, global_time, action_strength)
                        )
                        if target in state.pulses:
                            state.pulses[target].inbox.append(sig)
                            print(f" Emitted [{sig.name}] from [{self.name}] to [{target}] at t={global_time}")
                        signals_emitted.append(sig)
                        state.signals.append(sig)
                elif action.type == "broadcast" and action.signal:
                    for target in state.pulses:
                        if target != self.name:
                            sig = Signal(
                                from_agent=self.name,
                                name=action.signal,
                                time=global_time,
                                strength=action_strength,
                                signature=self.sign_signal(action.signal, global_time, action_strength)
                            )
                            state.pulses[target].inbox.append(sig)
                            print(f" Broadcasted [{action.signal}] from [{self.name}] to [{target}] at t={global_time}")
                            signals_emitted.append(sig)
                            state.signals.append(sig)
                elif action.type == "modulate" and action.phase_shift is not None:
                    if action.to in state.pulses:
                        state.pulses[action.to].next_fire += action.phase_shift * action_strength
                        print(f" Modulated [{action.to}] by {action.phase_shift} from [{self.name}] at t={global_time}")
        for signal in self.inbox:
            self.on_signal(signal, state, ledger)
        self.inbox.clear()
        return signals_emitted

    def evaluate_condition(self, condition: str) -> tuple[bool, float]:
        if self.bci_adapter:
            brain_state, strength = self.bci_adapter.interpret(self.bci_adapter.get_current_state())
            return brain_state == condition, strength
        return True, 1.0

    def sign_signal(self, signal_name: str, timestamp: float, strength: float) -> bytes:
        msg = f"{self.name}:{signal_name}:{timestamp}:{strength}".encode()
        return self.signing_key.sign(msg).signature

    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "interval": self.interval,
            "next_fire": self.next_fire,
            "body": [{"type": a.type, "signal": a.signal, "to": a.to, "phase_shift": a.phase_shift,
                      "fraction": a.fraction, "condition": a.condition, "strength": a.strength} for a in self.body],
            "time_scale": self.time_scale,
            "fractions": self.fractions,
            "enabled": self.enabled
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Pulse':
        return cls(**data)

class QuantumPulse(Pulse):
    def __init__(self, *args, state_vector: Optional[Dict[str, float]] = None, **kwargs):
        super().__init__(*args, **kwargs)
        self.state_vector = state_vector or {"on": 0.5, "off": 0.5}

    def should_fire(self, global_time: float) -> bool:
        if not self.enabled:
            return False
        collapsed_state, strength = self.collapse_state()
        should_fire = collapsed_state == "on"
        self.logger.log(
            "pulse_check",
            str(global_time).encode(),
            b"firing" if should_fire else b"not_firing",
            {"pulse_name": self.name, "state": collapsed_state, "strength": strength},
            "INFO",
            "PULSE_FIRING" if should_fire else "PULSE_NOT_READY"
        )
        return should_fire

    def collapse_state(self) -> tuple[str, float]:
        rand = random.random()
        cumulative = 0.0
        for state, prob in self.state_vector.items():
            cumulative += prob
            if rand <= cumulative:
                strength = prob if state == "on" else 1.0 - prob
                return state, min(1.0, max(0.0, strength))
        return "off", 0.5

@dataclass
class State:
    time: float = 0.0
    pulses: Dict[str, Any] = field(default_factory=dict)
    signals: List[Signal] = field(default_factory=list)

    def save_state(self, filename: str) -> None:
        state_data = {
            "time": self.time,
            "pulses": {name: pulse.to_dict() for name, pulse in self.pulses.items()},
            "signals": [{"from_agent": s.from_agent, "name": s.name, "time": s.time, "strength": s.strength,
                         "signature": base64.b64encode(s.signature).decode() if s.signature else None}
                        for s in self.signals]
        }
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(state_data, f, indent=2)

    def load_state(self, filename: str) -> None:
        if not os.path.exists(filename):
            return
        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.time = data.get("time", 0.0)
            self.pulses = {name: Pulse.from_dict(pulse_data) if not pulse_data.get("state_vector") else
                           QuantumPulse.from_dict(pulse_data) for name, pulse_data in data.get("pulses", {}).items()}
            self.signals = [
                Signal(
                    from_agent=s["from_agent"],
                    name=s["name"],
                    time=s["time"],
                    strength=s.get("strength", 1.0),
                    signature=base64.b64decode(s["signature"]) if s["signature"] else None
                ) for s in data.get("signals", [])
            ]

# === Blockchain ===
class SovereignBlockchain:
    def __init__(self, ledger: TransparencyLedger, chain_file: Path = CHAIN_FILE):
        self.ledger = ledger
        self.chain_file = chain_file
        self.chain = self.load_chain()
        self.difficulty = 2

    def load_chain(self):
        if self.chain_file.exists():
            with open(self.chain_file, "r") as f:
                return json.load(f)
            try:
                return json.loads(f.read())
            except json.JSONDecodeError:
                logger.error("Corrupted chain file, initializing new chain")
                return [{"genesis": "0" * 64, "transactions": [], "timestamp": int(time.time()), "hash": "0" * 64, "nonce": 0}]
        return [{"genesis": "0" * 64, "transactions": [], "timestamp": int(time.time()), "hash": "0" * 64, "nonce": 0}]

    def save_chain(self):
        with open(self.chain_file, "w") as f:
            json.dump(self.chain, f, indent=2)

    def compute_block_hash(self, previous_block_hash: str, transactions: List[Dict], nonce: int) -> str:
        data = previous_block_hash + json.dumps(transactions, sort_keys=True) + str(nonce)
        return hashlib.sha256(data.encode()).hexdigest()

    def is_valid_hash(self, hash_: str) -> bool:
        return hash_.startswith("0') * self.difficulty)

    def mine_block(self, transactions: List[Dict]) -> Dict:
        previous_block = self.chain[-1]
        previous_block_hash = previous_block.get("hash", previous_block.get("genesis", "0" * 64))
        nonce = 0
        while True:
            block_hash = self.compute_block_hash(previous_block_hash, transactions, nonce)
            if self.is_valid_hash(block_hash):
                block = {
                    "previous_block_hash": previous_block_hash,
                    "transactions": transactions,
                    "timestamp": time.time(),
                    "hash": block_hash,
                    "nonce": nonce
                }
                self.chain.append(block)
                self.save_chain()
                self.ledger.add_entry(
                    "blockchain", "block_mined",
                    {"block_hash": block_hash, "nonce": nonce, "transactions": transactions},
                    f"Mined block with hash {block_hash} at t={block['timestamp']}"
                logger.info(f"Mined block: {block_hash})")
                return block
            nonce += 1

# === Sovereign AI Partner ===
class SovereignAIPartner:
    def __init__(self, blockchain: SovereignBlockchain, state: State, dsl_interpreter: QPulseDSLInterpreter):
        self.blockchain = blockchain
        self.state = state
        self.dsl_interpreter = dsl._interpreter
        self.covenant_log = []
        self.bci_adapter = BCIAdapter(MockEEG())
        self.signing_key = nacl.signing.SigningKey.generate()

    def interact(self):
        print(" Sovereign AI Partner: Welcome to the Quantum Pulse Covenant Network")
        while True:
            print("\nActions: [create_pulse, collapse_node, emit_signal, view_ledger, form_covenant, view_covenant_log, mine_block, exit]")
            action = input("Choose action: ").lower()
            try:
                if action == "exit":
                    break
                elif action == "create_pulse":
                    self._create_pulse()
                elif action == "collapse_node":
                    self._collapse_node()
                elif action == "emit_signal":
                    self._emit_signal()
                elif action == "view_ledger":
                    self._view_ledger()
                elif action == "form_covenant":
                    self._form_covenant()
                elif action == "view_covenant_log":
                    self._view_covenant_log()
                elif action == "mine_block":
                    self._mine_block()
                else:
                    print("Invalid action")
            except Exception as e:
                print(f"Error: {e}")
                logger.error(f"Action failed: {str(e)}")

    def _create_pulse(self):
        name = input("Pulse name: ")
        interval = float(input("Enter interval (seconds): "))
        is_quantum = input("Is quantum pulse? (y/n): ").lower() == "y"
        states = input("Enter states (comma-separated, e.g., on,off): ").split(",")
        if is_quantum:
            state_vector = {s: 1.0 / len(states) for s in states}
            pulse = QuantumPulse(
                name=name,
                interval=interval,
                next_fire=interval,
                body=[],
                state_vector=state_vector,
                bci_adapter=self.bci_adapter
            )
        else:
            pulse = PulseNode(
                name=name,
                interval=interval,
                next_fire=interval,
                body=[],
                bci_enabled=Truebci_adapter=self._bci_adapter
            )
        self.state.pulses[name] = pulse
        self.dsl_interpreter.nodes[name] = {"states": states, "current": None}
        self.blockchain.ledger.add_entry(
            name, "pulse_created",
            {"interval": interval, "is_quantum": is_quantum, "states": "states},
            f"Created pulse {name} with states {states}"
        )
        self._log_covenant_action(f"Created pulse {name}")
        print(f"Pulse {name} created")

    def _collapse_node(self):
        node_name = input("Enter node name: ")
        intent = input("Enter intent (e.g., align): ")
        dsl_command = f"collapse {node_name} when intent == \"{intent}\" -> {intent} otherwise -> off"
        try:
            state = self._dsl_interpreter.execute(dsl_command)
            self._log_covenant_action(f"Collapsed node {node_name} to {state} with intent {intent}")
            print(f"Node {node_name} collapsed to {state}")
        except ValueError as e:
            print(f"Error: {e}")

    def _emit_signal(self):
        source = input("Source pulse: ")
        target = input("Target pulse: ")
        signal = input("Signal name:" ")
        time_str = input("Enter time (e.g., t=8.0): ") or f"t={time.time()}"
        dsl_command = f"emit {source} -> {target} : {signal} @ {time_str}"
        try:
            self._dsl_interpreter.execute(dsl_command)
            self._log_covenant_action(f"Emitted signal {signal} from {source} to {target}")
            print(f"Signal {signal} emitted")
        except ValueError as e:
            print(f"Error: {e}")

    def _view_ledger(self):
        for entry in self.blockchain._ledger.entries:
            print(f"t={entry.timestamp}: {entry.pulse_name} ({entry.operation}) - {entry.description}")
            print(f"  Verified: {self.blockchain.ledger._verify_entry(entry)}")

    def _form_covenant(self):
        node1 = input("First node: ")
        node2 = input("Second node: ")
        covenant = input("Covenant name: ")
        dsl_command = f"bond {node1} <-> {node2} via \"{covenant}\""
        try:
            self._dsl_interpreter.execute(dsl_command)
            self._log_covenant_action(f"Formed covenant {covenant} between {node1} and {node2}")
            print(f"Covenant {covenant} formed")
        except ValueError as e:
            print(f"Error: {e}")

    def _view_covenant_log(self):
        print("Covenant Log:")
        for entry in self.covenant_log:
            print(f"t={entry['timestamp']}: {entry['action']}")

    def _mine_block(self):
        transactions = []
        for signal in self.state.signals[-10:]:  # Last 10 signals
            transactions.append({
                "from_agent": signal.from_agent,
                "name": signal.name,
                "time": signal.time,
                "strength": signal.strength
            })
        block = self.blockchain.mine_block(transactions)
        self._log_covenant_action(f"Mined block {block['hash']}")
        print(f"Block mined: {block['hash']}")

    def _log_covenant_action(self, action: str):
        entry = {"timestamp": time.time(), "action": action}
        self.covenant_log.append(entry)
        self.blockchain.ledger.add_entry(
            "covenant", "log_action",
            entry,
            f"Covenant action: {action}"
        )

# === Mock EEG Source ===
class MockEEGSource:
    def read_brainwave_data(self) -> Dict[str, Any]:
        return {
            "alpha": random.uniform(0.0, 1.0),
            "beta": random.uniform(0.0, 1.0)
        }

# === Main Execution ===
if __name__ == "__main__":
    random.seed(42)
    ledger = TransparencyLedger()
    state = State()
    dsl_interpreter = QPulseDSLInterpreter(state, ledger)
    # Load state and ledger
    state.load_state(STATE_FILE)
    ledger.load_ledger(LEDGER_FILE, clear_if_invalid=True)

    # Initialize blockchain
    blockchain = SovereignBlockchain(ledger)

    # Load pulse configuration
    if os.path.exists(CONFIG_FILE):
        pulses = load_pulse_config(CONFIG_FILE")
        for pulse in pulses:
            with open(CONFIG_FILE, 'r") as f:
                config = json.load(f)
            pulses = []
            for p in config.get("pulses", []):
                if p.get("is_quantum", False)):
                    pulse = QuantumPulse(
                        name=p["name"],
                        interval=p["interval"],
                        next_fire=p["next_fire"],
                        body=[Action(**action) for action in p["body"]]],
                        time_scale=p.get("time_scale", 1.0),
                        fractions=p.get("fractions", "1),
                        enabled=p.get("enabled", True),
                        state_vector=p.get("state_vector", {"on": 0.5, "off": 0.5})
                    )
                else:
                    pulse_data = Pulse(
                        name=p["name"],
                        interval=p["interval"],
                        next_fire=p["next_fire"],
                        body=[Action(**action) for action in p["body"]],
                        time_scale=p.get("time_scale", 1.0),
                        fractions=p.get("fractions", "1),
                        enabled=p.get("enabled", True)
                    )
                pulses.append(pulse_data)
                if any(action.condition for action in pulse.body):
                    pulse.bci_enabled = BCIAdapter(MockEEG())
                state.pulses[pulse.name] = pulse_data
    else:
        state.pulses["alpha"] = Pulse(
            name="alpha",
            interval=4.0,
            next_fire=4.0,
            fractions=1,
            body=[
                Action(type="emit", signal="sync", to="beta", strength=0.8),
                Action(type="modulate", phase_shift=1.0, to="gamma", strength=0.0.9)
            ],
            bci_enabled=TruebciAdapter(MockEEGSource())
        )
        state.pulses["beta"] = QuantumPulse(
            name="beta",
            interval=3.0,
            next_fire=3.0,
            fractions=1,
            body=[
                Action(type="emit", signal="beta_ping", to="gamma", condition="focused", strength=0.7),
                Action(type="emit", signal="off", to="gamma", strength=1.0)
            ],
            state_vector={"on": 0.7, "off": 0.3},
            bci_enabled=TruebciAdapter(MockEEGSource())
        )
        state.pulses["gamma"] = Pulse(
            name="gamma",
            interval=5.0,
            next_fire=5.0,
            fractions=1,
            body=[
                Action(type="broadcast", signal="gamma_wave", condition="neutral", strength=0.6)
            ],
            bci_enabled=TruebciAdapter(MockEEGSource())
        )
        config_file = {
            "pulses": [
                {**p.to_dict(), "is_quantum": isinstance(p, QuantumPulse), "state_vector": getattr(p, "state_vector", None)}
                for p in state.pulses.values()
            ]
        }
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2)

    # Initialize AI partner
    ai_partner = SovereignAIPartner(blockchain, state, dsl_interpreter)

    # Run simulation for 20 seconds
    state.time = 0.0
    while state.time <= 20.0:
        for name in list(state.pulses.keys()):
            pulse = state.pulses[name]
            signals = pulse.fire(state.time, state, ledger)
            for sig in signals:
                ledger.add_entry(
                    pulse.name, "fire",
                    {"signal": sig.name, "time": sig.time, "strength": sig.strength},
                    f"Signal {sig.name} emitted from {pulse.name} at time {sig.time}"
                )
        state.time += 1.0

    # Save state and logs
    state.save_state(STATE_FILE)
    for pulse in state.pulses.values():
        pulse.logger.save_log(DATA_DIR / f"{pulse.name}_log.json")
    ledger.save_ledger(LEDGER_FILE)

    # Start AI partner interaction
    ai_partner.interact()

    print("\n=== Ledger Verification ===")
    for entry in ledger.entries:
        print(f"Ledger entry for {entry.pulse_name} ({entry.operation}) verified: {ledger.verify_entry(entry)}")

    print("\n=== Final Signals ===")
    for sig in sorted(state.signals, key=lambda x: x.time):
        print(f" {sig.from_agent} -> {sig.name} at t={sig.time} (strength: {sig.strength})")